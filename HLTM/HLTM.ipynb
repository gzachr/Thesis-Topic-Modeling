{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dnaso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dnaso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import re\n",
    "import nltk\n",
    "import requests\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "# Make sure to download nltk stopwords if not already\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       video_id                                               tags  \\\n",
      "0   M0lKXfyJh-I  ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...   \n",
      "1   cgnBQ9qOqmY  ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...   \n",
      "2   kgG1oJiE5qs  ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...   \n",
      "3   LB20skQI1_k  ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...   \n",
      "51  EKNuv99XeCA  ['Chinese',, 'Sassa, Dagdag',, 'La, Vie, En, R...   \n",
      "\n",
      "                                          transcripts  \\\n",
      "0   This is Pinoy life,\\nHere in the Philippines,\\...   \n",
      "1   I \\nam now a total germaphobe,\\nAlways disinfe...   \n",
      "2   Manila, oh nana\\nI love to party with my frien...   \n",
      "3   It's Mikey b*tch!\\nAt the palengke (Filipino m...   \n",
      "51  Hello.my name is max, from china!\\nthanks for ...   \n",
      "\n",
      "                                                 text  \n",
      "0   ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...  \n",
      "1   ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...  \n",
      "2   ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...  \n",
      "3   ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...  \n",
      "51  ['Chinese',, 'Sassa, Dagdag',, 'La, Vie, En, R...  \n"
     ]
    }
   ],
   "source": [
    "# Define paths to transcripts and tags directories\n",
    "transcripts_dir = '../cleaned_transcripts/'\n",
    "tags_dir = '../tags/'\n",
    "csv_path = '../cleaned_results.xlsx'\n",
    "\n",
    "# Load the CSV file and filter for \"related\" videos\n",
    "csv_data = pd.read_excel(csv_path)\n",
    "related_videos = csv_data[csv_data[\"related\"] == \"yes\"]\n",
    "\n",
    "# Load transcripts\n",
    "transcripts = []\n",
    "tags = []\n",
    "\n",
    "# Process each related video based on its video_id\n",
    "for video_id in related_videos[\"Video Id\"]:\n",
    "    # Construct paths based on video ID naming conventions\n",
    "    transcript_file = os.path.join(transcripts_dir, f\"{video_id}_captions.txt\")\n",
    "    tag_file = os.path.join(tags_dir, f\"{video_id}.txt\")\n",
    "    \n",
    "    # Read the transcript and tag files if they exist\n",
    "    try:\n",
    "        with open(transcript_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            transcripts.append(file.read())\n",
    "        with open(tag_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            tags.append(file.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Files for video ID {video_id} not found, skipping.\")\n",
    "\n",
    "# Combine the filtered data into a DataFrame\n",
    "data = pd.DataFrame({\"video_id\": related_videos[\"Video Id\"], \"tags\": tags, \"transcripts\": transcripts})\n",
    "data[\"text\"] = data[\"tags\"] + \" \" + data[\"transcripts\"]\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcripts</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0lKXfyJh-I</td>\n",
       "      <td>['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...</td>\n",
       "      <td>This is Pinoy life,\\nHere in the Philippines,\\...</td>\n",
       "      <td>['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...</td>\n",
       "      <td>[mikey, bustos, pinoy, manila, comedy, lol, pi...</td>\n",
       "      <td>[para, tondo, ulam, mikey, parody, bustos, sil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cgnBQ9qOqmY</td>\n",
       "      <td>['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...</td>\n",
       "      <td>I \\nam now a total germaphobe,\\nAlways disinfe...</td>\n",
       "      <td>['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...</td>\n",
       "      <td>[mikey, bustos, pinoy, manila, comedy, lifesty...</td>\n",
       "      <td>[cover, vaccines, fluids, panic, coronavirus, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kgG1oJiE5qs</td>\n",
       "      <td>['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...</td>\n",
       "      <td>Manila, oh nana\\nI love to party with my frien...</td>\n",
       "      <td>['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...</td>\n",
       "      <td>[mikey, bustos, pinoy, manila, comedy, lifesty...</td>\n",
       "      <td>[hot, fort, bonifacio, cinnamon, shot, mabuhay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LB20skQI1_k</td>\n",
       "      <td>['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...</td>\n",
       "      <td>It's Mikey b*tch!\\nAt the palengke (Filipino m...</td>\n",
       "      <td>['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...</td>\n",
       "      <td>[mikey, bustos, pinoy, manila, comedy, lifesty...</td>\n",
       "      <td>[veggies, sending, pilipinas, plants, bicol, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>EKNuv99XeCA</td>\n",
       "      <td>['Chinese',, 'Sassa, Dagdag',, 'La, Vie, En, R...</td>\n",
       "      <td>Hello.my name is max, from china!\\nthanks for ...</td>\n",
       "      <td>['Chinese',, 'Sassa, Dagdag',, 'La, Vie, En, R...</td>\n",
       "      <td>[chinese, sassa, dagdag, vie, en, rose, 107, b...</td>\n",
       "      <td>[feel, 107, lyrics, coz, language, chinese, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>AI7Us_5-IaQ</td>\n",
       "      <td>['El, Nido',, 'Palawan',, 'philippines',, 'isl...</td>\n",
       "      <td>We've got to El Nido, the weather is terrible ...</td>\n",
       "      <td>['El, Nido',, 'Palawan',, 'philippines',, 'isl...</td>\n",
       "      <td>[nido, palawan, island, hopping, island, backp...</td>\n",
       "      <td>[walking, scuba, lagoons, cheers, barefoot, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>dbdGpkHLx9Y</td>\n",
       "      <td>['coron',, 'coron, palawan',, 'coron, philippi...</td>\n",
       "      <td>Welcome to Ocam ocam beach. To get here you ca...</td>\n",
       "      <td>['coron',, 'coron, palawan',, 'coron, philippi...</td>\n",
       "      <td>[coron, coron, palawan, coron, malcapuya, beac...</td>\n",
       "      <td>[beaches, saltwater, lagoon, palawan, safari, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>V-r2M6E4iDM</td>\n",
       "      <td>['red, rock, waterfall',, 'pulang, bato',, 'pu...</td>\n",
       "      <td>welcome to dumaguete\\nalso known as the city o...</td>\n",
       "      <td>['red, rock, waterfall',, 'pulang, bato',, 'pu...</td>\n",
       "      <td>[red, rock, waterfall, pulang, bato, pulang, b...</td>\n",
       "      <td>[motorcycle, result, tourists, presence, drive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>8X_d-IVydP4</td>\n",
       "      <td>[]</td>\n",
       "      <td>foreign\\n[Music]\\ngood morning it is half past...</td>\n",
       "      <td>[] foreign\\n[Music]\\ngood morning it is half p...</td>\n",
       "      <td>[morning, woken, nido, day, nido, plan, action...</td>\n",
       "      <td>[ping, calmer, film, island, paradise, fresh, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>y3EZ-yeGqyk</td>\n",
       "      <td>['bellevue, resort',, 'dakak, resort',, '5, st...</td>\n",
       "      <td>[Music]\\nthank you\\n[Music]\\n[Applause]\\n[Musi...</td>\n",
       "      <td>['bellevue, resort',, 'dakak, resort',, '5, st...</td>\n",
       "      <td>[bellevue, resort, dakak, resort, star, resort...</td>\n",
       "      <td>[luxury, panglao, del, villa, zamboanga, 4k, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                               tags  \\\n",
       "0     M0lKXfyJh-I  ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...   \n",
       "1     cgnBQ9qOqmY  ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...   \n",
       "2     kgG1oJiE5qs  ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...   \n",
       "3     LB20skQI1_k  ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...   \n",
       "51    EKNuv99XeCA  ['Chinese',, 'Sassa, Dagdag',, 'La, Vie, En, R...   \n",
       "...           ...                                                ...   \n",
       "2080  AI7Us_5-IaQ  ['El, Nido',, 'Palawan',, 'philippines',, 'isl...   \n",
       "2081  dbdGpkHLx9Y  ['coron',, 'coron, palawan',, 'coron, philippi...   \n",
       "2082  V-r2M6E4iDM  ['red, rock, waterfall',, 'pulang, bato',, 'pu...   \n",
       "2083  8X_d-IVydP4                                                 []   \n",
       "2084  y3EZ-yeGqyk  ['bellevue, resort',, 'dakak, resort',, '5, st...   \n",
       "\n",
       "                                            transcripts  \\\n",
       "0     This is Pinoy life,\\nHere in the Philippines,\\...   \n",
       "1     I \\nam now a total germaphobe,\\nAlways disinfe...   \n",
       "2     Manila, oh nana\\nI love to party with my frien...   \n",
       "3     It's Mikey b*tch!\\nAt the palengke (Filipino m...   \n",
       "51    Hello.my name is max, from china!\\nthanks for ...   \n",
       "...                                                 ...   \n",
       "2080  We've got to El Nido, the weather is terrible ...   \n",
       "2081  Welcome to Ocam ocam beach. To get here you ca...   \n",
       "2082  welcome to dumaguete\\nalso known as the city o...   \n",
       "2083  foreign\\n[Music]\\ngood morning it is half past...   \n",
       "2084  [Music]\\nthank you\\n[Music]\\n[Applause]\\n[Musi...   \n",
       "\n",
       "                                                   text  \\\n",
       "0     ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...   \n",
       "1     ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...   \n",
       "2     ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...   \n",
       "3     ['Mikey, Bustos',, 'Pinoy',, 'Filipino',, 'Phi...   \n",
       "51    ['Chinese',, 'Sassa, Dagdag',, 'La, Vie, En, R...   \n",
       "...                                                 ...   \n",
       "2080  ['El, Nido',, 'Palawan',, 'philippines',, 'isl...   \n",
       "2081  ['coron',, 'coron, palawan',, 'coron, philippi...   \n",
       "2082  ['red, rock, waterfall',, 'pulang, bato',, 'pu...   \n",
       "2083  [] foreign\\n[Music]\\ngood morning it is half p...   \n",
       "2084  ['bellevue, resort',, 'dakak, resort',, '5, st...   \n",
       "\n",
       "                                           cleaned_text  \\\n",
       "0     [mikey, bustos, pinoy, manila, comedy, lol, pi...   \n",
       "1     [mikey, bustos, pinoy, manila, comedy, lifesty...   \n",
       "2     [mikey, bustos, pinoy, manila, comedy, lifesty...   \n",
       "3     [mikey, bustos, pinoy, manila, comedy, lifesty...   \n",
       "51    [chinese, sassa, dagdag, vie, en, rose, 107, b...   \n",
       "...                                                 ...   \n",
       "2080  [nido, palawan, island, hopping, island, backp...   \n",
       "2081  [coron, coron, palawan, coron, malcapuya, beac...   \n",
       "2082  [red, rock, waterfall, pulang, bato, pulang, b...   \n",
       "2083  [morning, woken, nido, day, nido, plan, action...   \n",
       "2084  [bellevue, resort, dakak, resort, star, resort...   \n",
       "\n",
       "                                         filtered_words  \n",
       "0     [para, tondo, ulam, mikey, parody, bustos, sil...  \n",
       "1     [cover, vaccines, fluids, panic, coronavirus, ...  \n",
       "2     [hot, fort, bonifacio, cinnamon, shot, mabuhay...  \n",
       "3     [veggies, sending, pilipinas, plants, bicol, b...  \n",
       "51    [feel, 107, lyrics, coz, language, chinese, so...  \n",
       "...                                                 ...  \n",
       "2080  [walking, scuba, lagoons, cheers, barefoot, of...  \n",
       "2081  [beaches, saltwater, lagoon, palawan, safari, ...  \n",
       "2082  [motorcycle, result, tourists, presence, drive...  \n",
       "2083  [ping, calmer, film, island, paradise, fresh, ...  \n",
       "2084  [luxury, panglao, del, villa, zamboanga, 4k, c...  \n",
       "\n",
       "[1674 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Text Preprocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Function to fetch stopwords from GitHub URL\n",
    "def fetch_stopwords_from_github(url):\n",
    "    response = requests.get(url)\n",
    "    github_stopwords = response.text.splitlines()  # Split by new lines\n",
    "    return set(github_stopwords)\n",
    "\n",
    "# GitHub URL for stopwords\n",
    "github_stopwords_url = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt'\n",
    "github_stopwords = fetch_stopwords_from_github(github_stopwords_url)\n",
    "\n",
    "\n",
    "custom_stop_words = ['like', 'yeah', 'know', 'um', 'uh', 'really', 'one', 'go', 'right', 'okay', 'well', 'said', \n",
    "                     'going', 'got', 'na', 'always', 'every', 'each', 'say', 'el', 'little', 'still', \n",
    "                     'best', 'dutch', 'nice', 'great', 'awesome', 'good', 'cool', 'love', 'amazing', 'wow' ]\n",
    "broad_terms = ['philippines', 'philippine', 'british', 'filipino', 'video', 'http', 'korea', 'korean', \n",
    "               'youtube', 'google', 'united', 'america', 'american']\n",
    "kpop_keywords = ['kpop', '필리핀', 'bts', 'blackpink', 'twice', 'exo', 'k-pop', 'seventeen', \n",
    "                 'stray kids', 'nct', 'kdrama', 'aespa', 'taehyung', 'jimin', 'jungkook']\n",
    "more_keywords = [\n",
    "    'breaking news', 'report', 'coverage', 'investigation', 'interview', 'documentary', \n",
    "    'journalist', 'headline', 'reporter', 'current events', 'special report', \n",
    "    'analysis', 'documented', 'broadcast', 'reporting', 'v', 'food', 'travel', 'react', \n",
    "    'reacts', 'reaction', 'foreigner', 'thing', 'visit', 'dc', 'japan', 'first', 'fast', \n",
    "    'asia', 'ang', 'indian', 'thai', 'vietnamese', 'russia', 'gon', 'canada', 'canadian', 'russian', \n",
    "    'russia', 'guy', 'lot', 'bit', 'diba', 'ola', 'cuz', 'thai', 'thailand', 'person', 'citizen', 'foreigner', 'foreign', 'foreigners',\n",
    "    'facebook', 'filipinos', 'filipinas', 'vlog', 'vlogs', 'vlogging', 'hashtag', 'india', 'bro', 'dito', 'people', 'time', 'music', 'guys'\n",
    "]\n",
    "\n",
    "# Add custom and broad terms\n",
    "stop_words.update(custom_stop_words, broad_terms, kpop_keywords, more_keywords, github_stopwords)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and remove non-alphabet characters\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords and filter out short words\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 1]\n",
    "    return words\n",
    "\n",
    "data[\"cleaned_text\"] = data[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Vectorize text with TF-IDF to remove low-impact words\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, min_df=5)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([\" \".join(doc) for doc in data[\"cleaned_text\"]])\n",
    "\n",
    "# Filter words in each transcript based on TF-IDF scores\n",
    "def filter_by_tfidf(doc):\n",
    "    features = tfidf_vectorizer.get_feature_names_out()\n",
    "    vector = tfidf_vectorizer.transform([\" \".join(doc)]).toarray()[0]\n",
    "    return [features[i] for i in vector.argsort()[-15:]]  # Top 15 tf-idf terms\n",
    "\n",
    "data[\"filtered_words\"] = data[\"cleaned_text\"].apply(filter_by_tfidf)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created File Using only Cleaned Text\n",
    "with open('cleaned_aggregate.txt', 'w', encoding='utf-8') as file:\n",
    "    for row in data['cleaned_text']:\n",
    "        file.write(\" \".join(row).replace(',', '') + '\\n')\n",
    "\n",
    "# Created File using filtered text\n",
    "with open('filtered_aggregate.txt', 'w', encoding='utf-8') as file:\n",
    "    for row in data['filtered_words']:\n",
    "        file.write(\" \".join(row).replace(',', '') + '\\n')\n",
    "\n",
    "\n",
    "# to use HLTM:\n",
    "# java -cp HLTA.jar;HLTA-deps.jar tm.hlta.HTD \"file name of text\" \"output name\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
