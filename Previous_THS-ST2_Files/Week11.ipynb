{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import unicodedata\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures, TrigramCollocationFinder, TrigramAssocMeasures\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import product, combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Fetch stopwords\n",
    "def fetch_stopwords_from_github(url):\n",
    "    response = requests.get(url)\n",
    "    return set(response.text.splitlines())\n",
    "\n",
    "github_stopwords_url = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt'\n",
    "github_stopwords = fetch_stopwords_from_github(github_stopwords_url)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "with open(\"stop_words.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    custom_stop_words = set(file.read().splitlines())\n",
    "\n",
    "stop_words.update(custom_stop_words, github_stopwords)\n",
    "\n",
    "# Folder paths\n",
    "#transcripts_folder_path = './Previous_THS-ST2_Files/standard_dataset_old/'\n",
    "transcripts_folder_path = 'final_transcripts/'\n",
    "\n",
    "tags_folder_path = 'video_tags/'\n",
    "titles_folder_path = 'video_titles/'\n",
    "\n",
    "# Function to load video tags only for fetched video IDs\n",
    "def load_video_tags(folder_path, video_ids):\n",
    "    video_tags = {}\n",
    "    for video_id in video_ids:\n",
    "        tag_file = os.path.join(folder_path, f\"{video_id}.txt\")\n",
    "        if os.path.exists(tag_file):\n",
    "            with open(tag_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                tags_content = file.read().lower()\n",
    "                video_tags[video_id] = tags_content.split()  # Store as list of words\n",
    "        else:\n",
    "            video_tags[video_id] = []  # Default to empty list if no tags\n",
    "    return video_tags\n",
    "# Function to load video titles\n",
    "def load_video_titles(folder_path, video_ids):\n",
    "    video_titles = {}\n",
    "    for video_id in video_ids:\n",
    "        title_file = os.path.join(folder_path, f\"{video_id}\")\n",
    "        if os.path.exists(title_file):\n",
    "            with open(title_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                video_titles[video_id] = file.read().strip()  # Read full title\n",
    "        else:\n",
    "            video_titles[video_id] = \"Unknown Title\"  # Default if no title file\n",
    "    return video_titles\n",
    "\n",
    "video_ids = []\n",
    "transcript_files = []\n",
    "for file_name in os.listdir(transcripts_folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        video_id = file_name.split('_captions')[0]\n",
    "        video_ids.append(video_id)\n",
    "        transcript_files.append((video_id, file_name)) \n",
    "\n",
    "video_tags = load_video_tags(tags_folder_path, video_ids)\n",
    "video_titles = load_video_titles(titles_folder_path, video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_latin_script(word):\n",
    "    return all('LATIN' in unicodedata.name(char, '') or char.isdigit() for char in word)\n",
    "\n",
    "# Function to detect both bigram and trigram collocations\n",
    "def detect_collocations(tokens, min_freq=2):\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    trigram_measures = TrigramAssocMeasures()\n",
    "\n",
    "    # Find bigrams\n",
    "    bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "    bigram_finder.apply_freq_filter(min_freq)\n",
    "    bigrams = set(['_'.join(bigram) for bigram in bigram_finder.nbest(bigram_measures.pmi, 10)])\n",
    "\n",
    "    # Find trigrams\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(tokens)\n",
    "    trigram_finder.apply_freq_filter(min_freq)\n",
    "    trigrams = set(['_'.join(trigram) for trigram in trigram_finder.nbest(trigram_measures.pmi, 10)])\n",
    "\n",
    "    return bigrams, trigrams\n",
    "\n",
    "def is_valid_ngram(ngram, existing_ngrams):\n",
    "    \"\"\" Check if the n-gram contains alternating or duplicate words \"\"\"\n",
    "    words = ngram.split('_')\n",
    "    unique_words = set(words)\n",
    "    \n",
    "    if len(unique_words) == 1:\n",
    "        return False  \n",
    "    \n",
    "    if len(words) > 2 and words[0] == words[2]:  \n",
    "        return False  \n",
    "    \n",
    "    if len(words) > 2 and words[0] == words[1]:\n",
    "        return False\n",
    "\n",
    "    if len(words) == 3 and words[1] == words[2]:\n",
    "        return False \n",
    "    \n",
    "    if len(words) == 2:\n",
    "        for existing_ngram in existing_ngrams:\n",
    "            if ngram in existing_ngram:\n",
    "                return False  \n",
    "    \n",
    "    return True\n",
    "\n",
    "def preprocess_text(doc, video_id, tag_weight=2, ngram_weight_factor=3):\n",
    "    # Clean punctuation at the end of words\n",
    "    doc = re.sub(r'([a-zA-Z]+)[,;:!?.]', r'\\1', doc)\n",
    "\n",
    "    # Lowercase and remove punctuation\n",
    "    doc = doc.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(doc)\n",
    "\n",
    "    # Remove stopwords & non-latin words\n",
    "    tokens = [word for word in tokens if word not in stop_words and word.isalpha() and is_latin_script(word)]\n",
    "\n",
    "    # POS tagging\n",
    "    tokens_with_pos = pos_tag(tokens)\n",
    "\n",
    "    # Remove verbs and adjectives before n-gram detection\n",
    "    pos_exclude = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS'}\n",
    "    filtered_tokens = [word for word, pos in tokens_with_pos if pos not in pos_exclude]\n",
    "\n",
    "    # Include video tags in filtered tokens\n",
    "    if video_id in video_tags:\n",
    "        tags = video_tags[video_id]\n",
    "        cleaned_tags = []\n",
    "        \n",
    "        for tag in tags:\n",
    "            tag = re.sub(r'^[,;:!?.\\'\\\"]*([a-zA-Z]+)[,;:!?.\\'\\\"]*$', r'\\1', tag)\n",
    "            tag = tag.lower().translate(str.maketrans('', '', string.punctuation))  \n",
    "            if tag.isalpha() and tag not in stop_words and is_latin_script(tag):  \n",
    "                cleaned_tags.append(tag)\n",
    "\n",
    "        filtered_tokens.extend(cleaned_tags)  \n",
    "\n",
    "    # Detect meaningful bigram and trigram collocations (tags are included)\n",
    "    bigrams, trigrams = detect_collocations(filtered_tokens)\n",
    "\n",
    "    # Generate n-grams\n",
    "    bigram_tokens = ['_'.join(gram) for gram in ngrams(filtered_tokens, 2)]\n",
    "    trigram_tokens = ['_'.join(gram) for gram in ngrams(filtered_tokens, 3)]\n",
    "\n",
    "    # Remove invalid n-grams (duplicates, alternating patterns)\n",
    "    bigram_tokens = [bigram for bigram in bigram_tokens if is_valid_ngram(bigram, set())]\n",
    "    trigram_tokens = [trigram for trigram in trigram_tokens if is_valid_ngram(trigram, bigram_tokens)]\n",
    "\n",
    "    # Keep only meaningful n-grams from detect_collocations()\n",
    "    bigram_tokens = [bigram for bigram in bigram_tokens if bigram in bigrams]\n",
    "    trigram_tokens = [trigram for trigram in trigram_tokens if trigram in trigrams]\n",
    "\n",
    "    # Remove n-grams that are just reordered versions\n",
    "    unique_ngrams = set()\n",
    "    filtered_bigrams = []\n",
    "    filtered_trigrams = []\n",
    "\n",
    "    for bigram in bigram_tokens:\n",
    "        sorted_bigram = '_'.join(sorted(bigram.split('_')))\n",
    "        if sorted_bigram not in unique_ngrams:\n",
    "            unique_ngrams.add(sorted_bigram)\n",
    "            filtered_bigrams.append(bigram)\n",
    "\n",
    "    for trigram in trigram_tokens:\n",
    "        sorted_trigram = '_'.join(sorted(trigram.split('_')))\n",
    "        if sorted_trigram not in unique_ngrams:\n",
    "            unique_ngrams.add(sorted_trigram)\n",
    "            filtered_trigrams.append(trigram)\n",
    "    \n",
    "    # Count n-gram frequency\n",
    "    bigram_frequencies = Counter(filtered_bigrams)\n",
    "    trigram_frequencies = Counter(filtered_trigrams)\n",
    "\n",
    "    # Merge n-grams into single tokens\n",
    "    bigram_trigram_words = set()\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(filtered_tokens) - 2:  # Check for trigrams first\n",
    "        trigram = f\"{filtered_tokens[i]}_{filtered_tokens[i+1]}_{filtered_tokens[i+2]}\"\n",
    "        bigram = f\"{filtered_tokens[i]}_{filtered_tokens[i+1]}\"\n",
    "\n",
    "        if trigram in filtered_trigrams:\n",
    "            merged_tokens.append(trigram)\n",
    "            bigram_trigram_words.add(trigram)\n",
    "            i += 3  # Skip next two words since it's part of the trigram\n",
    "        elif bigram in filtered_bigrams:\n",
    "            merged_tokens.append(bigram)\n",
    "            bigram_trigram_words.add(bigram)\n",
    "            i += 2  # Skip next word since it's part of the bigram\n",
    "        else:\n",
    "            merged_tokens.append(filtered_tokens[i])\n",
    "            i += 1\n",
    "    \n",
    "    # Append any remaining words\n",
    "    while i < len(filtered_tokens):\n",
    "        merged_tokens.append(filtered_tokens[i])\n",
    "        i += 1\n",
    "\n",
    "    # Store n-gram components to remove single tokens later\n",
    "    ngram_components = set(word for bigram in bigram_tokens for word in bigram.split('_'))\n",
    "    ngram_components.update(word for trigram in trigram_tokens for word in trigram.split('_'))\n",
    "\n",
    "    # Remove single tokens if they appear in any n-gram\n",
    "    filtered_tokens = [word for word in filtered_tokens if word not in ngram_components]\n",
    "\n",
    "    # Remove duplicates before assigning weight\n",
    "    unique_tokens = list(set(merged_tokens))\n",
    "\n",
    "    # Assign weight based on n-gram occurrence\n",
    "    weighted_tokens = []\n",
    "    for token in unique_tokens:\n",
    "        if token in trigram_frequencies:\n",
    "            token_weight = trigram_frequencies[token] * 2 + ngram_weight_factor  \n",
    "        elif token in bigram_frequencies:\n",
    "            token_weight = bigram_frequencies[token] * ngram_weight_factor\n",
    "        else:\n",
    "            token_weight = 1\n",
    "        weighted_tokens.extend([token] * int(token_weight))\n",
    "\n",
    "    # # Include video tags with weight\n",
    "    # if video_id in video_tags:\n",
    "    #     for tag in cleaned_tags:\n",
    "    #         weighted_tokens.extend([tag] * int(tag_weight))\n",
    "\n",
    "    return ' '.join(weighted_tokens), list(bigram_trigram_words)\n",
    "\n",
    "def topic_diversity(model, top_n=10):\n",
    "    topic_words = [set([word for word, _ in model.show_topic(topic_id, top_n)]) for topic_id in range(model.num_topics)]\n",
    "    unique_words = set().union(*topic_words)\n",
    "    return len(unique_words) / (top_n * len(topic_words))\n",
    "\n",
    "def jaccard_similarity(topic1, topic2):\n",
    "    return len(set(topic1) & set(topic2)) / len(set(topic1) | set(topic2))\n",
    "\n",
    "def avg_jaccard_similarity(model, num_words=10):\n",
    "    topics = [set([word for word, _ in model.show_topic(topic_id, num_words)]) for topic_id in range(model.num_topics)]\n",
    "    similarities = [jaccard_similarity(t1, t2) for t1, t2 in combinations(topics, 2)]\n",
    "    return sum(similarities) / len(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = []\n",
    "preprocessed_text = []\n",
    "bigram_trigram_text = {}\n",
    "\n",
    "for video_id, file_name in transcript_files:\n",
    "    with open(os.path.join(transcripts_folder_path, file_name), 'r', encoding='utf-8') as file:\n",
    "        content = file.read().lower()\n",
    "        if len(content.split()) >= 100: \n",
    "            processed_text, bigram_trigram = preprocess_text(content, video_id)\n",
    "            preprocessed_text.append((video_id, processed_text))\n",
    "            all_documents.append(processed_text)\n",
    "            bigram_trigram_text[video_id] = bigram_trigram\n",
    "\n",
    "all_tokens = [token for doc in all_documents for token in doc.split()]\n",
    "token_freq = Counter(all_tokens)\n",
    "\n",
    "high_freq_tokens = {token for token, freq in token_freq.items() if freq > 200 and '_' not in token}\n",
    "\n",
    "filtered_documents_with_id = []\n",
    "for video_id, doc in preprocessed_text:\n",
    "    filtered_doc = [token for token in doc.split() if token not in high_freq_tokens]\n",
    "    filtered_documents_with_id.append((video_id, filtered_doc))  \n",
    "\n",
    "filtered_documents_only = [doc for _, doc in filtered_documents_with_id]\n",
    "\n",
    "filtered_documents = []\n",
    "for doc in all_documents:\n",
    "    filtered_doc = [token for token in doc.split() if token not in high_freq_tokens]\n",
    "    filtered_documents.append(filtered_doc)\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(filtered_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in filtered_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPreprocessed Text Per Video:\")\n",
    "for video_id, processed_text in preprocessed_text:\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\") \n",
    "    print(f\"\\nVideo ID: {video_id} | {video_title}\\n- {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_high_freq_tokens = sorted(high_freq_tokens, key=lambda token: token_freq[token], reverse=True)\n",
    "for token in sorted_high_freq_tokens:\n",
    "    print(token + \":\", token_freq[token])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq_dict = {dictionary[id]: freq for id, freq in dictionary.cfs.items()}\n",
    "\n",
    "print(\"Top 100 Most Frequent Tokens:\")\n",
    "sorted_tokens = sorted(token_freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for token, freq in sorted_tokens[:1000]:\n",
    "    print(f\"{token}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in video_ids:\n",
    "    video_title = video_titles.get(n, \"Unknown Title\")\n",
    "    print(\"Video ID: \", n, video_title)\n",
    "    \n",
    "    if n in bigram_trigram_text:  \n",
    "        print(f\"{bigram_trigram_text[n]}\")\n",
    "    else:\n",
    "        print(\"No bigram/trigram data available for this video.\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train LDA Model\n",
    "# # lda_model_12 = LdaModel(corpus, num_topics=20, id2word=dictionary, alpha='auto', eta='auto', passes=100, random_state=42)\n",
    "\n",
    "# # # Compute Coherence Score\n",
    "# # coherence_model = CoherenceModel(model=lda_model_12, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "# # coherence_score = coherence_model.get_coherence()\n",
    "# # print(f\"Coherence Score: {coherence_score}\")\n",
    "\n",
    "# num_topics_range = [15, 20, 25, 30] \n",
    "# alpha_values = ['symmetric', 'asymmetric', 0.01, 0.1, 0.5, 'auto']\n",
    "# eta_values = ['symmetric', 0.01, 0.1, 0.5, 'auto']\n",
    "# results = []\n",
    "\n",
    "# # Grid Search\n",
    "# for num_topics, alpha, eta in product(num_topics_range, alpha_values, eta_values):\n",
    "#     print(f\"Training LDA Model with num_topics={num_topics}, alpha={alpha}, eta={eta}...\")\n",
    "    \n",
    "#     # Train LDA Model\n",
    "#     lda_model = LdaModel(\n",
    "#         corpus=corpus,\n",
    "#         id2word=dictionary,\n",
    "#         num_topics=num_topics,\n",
    "#         alpha=alpha,\n",
    "#         eta=eta,\n",
    "#         passes=50,  \n",
    "#         random_state=42\n",
    "#     )\n",
    "\n",
    "#     # Compute Coherence Scores\n",
    "#     coherence_c_v = CoherenceModel(model=lda_model, texts=filtered_documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "#     coherence_u_mass = CoherenceModel(model=lda_model, corpus=corpus, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "#     coherence_npmi = CoherenceModel(model=lda_model, texts=filtered_documents, dictionary=dictionary, coherence='c_npmi').get_coherence()\n",
    "\n",
    "#     # Compute Topic Diversity\n",
    "#     diversity_score = topic_diversity(lda_model)\n",
    "\n",
    "#     # Compute Jaccard Similarity\n",
    "#     jaccard_score = avg_jaccard_similarity(lda_model)\n",
    "\n",
    "#     # Store results\n",
    "#     results.append({\n",
    "#         \"num_topics\": num_topics,\n",
    "#         \"alpha\": alpha,\n",
    "#         \"eta\": eta,\n",
    "#         \"coherence_c_v\": coherence_c_v,\n",
    "#         \"coherence_u_mass\": coherence_u_mass,\n",
    "#         \"coherence_npmi\": coherence_npmi,\n",
    "#         \"topic_diversity\": diversity_score,\n",
    "#         \"jaccard_similarity\": jaccard_score\n",
    "#     })\n",
    "\n",
    "# df_results = pd.DataFrame(results)\n",
    "\n",
    "# # Sort by best coherence and diversity balance\n",
    "# df_sorted = df_results.sort_values(by=[\"coherence_c_v\", \"topic_diversity\"], ascending=[False, False])\n",
    "# print(df_sorted.head(10))  # Top 10 models\n",
    "\n",
    "# # Print Topics\n",
    "# # topics = lda_model_12.print_topics(num_words=100)\n",
    "# # for topic_id, topic_words in topics:\n",
    "# #     print(f\"Topic {topic_id}: {topic_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results.to_csv(\"lda_hyperparameter_tuning_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the best parameters per number of topics\n",
    "# best_params_per_topic = df_results.loc[\n",
    "#     df_results.groupby(\"num_topics\")[\"coherence_c_v\"].idxmax()\n",
    "# ]\n",
    "\n",
    "# # Sort by num_topics for clarity\n",
    "# best_params_per_topic = best_params_per_topic.sort_values(by=\"num_topics\")\n",
    "\n",
    "# print(\"Best parameters per number of topics:\")\n",
    "# print(best_params_per_topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_30 = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=30,\n",
    "        alpha=0.1,\n",
    "        eta=0.1,\n",
    "        passes=50,  \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Compute Coherence Scores\n",
    "coherence_c_v = CoherenceModel(model=lda_model_30, texts=filtered_documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "coherence_u_mass = CoherenceModel(model=lda_model_30, corpus=corpus, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "coherence_npmi = CoherenceModel(model=lda_model_30, texts=filtered_documents, dictionary=dictionary, coherence='c_npmi').get_coherence()\n",
    "\n",
    "# Compute Topic Diversity\n",
    "diversity_score = topic_diversity(lda_model_30)\n",
    "\n",
    "# Compute Jaccard Similarity\n",
    "jaccard_score = avg_jaccard_similarity(lda_model_30)\n",
    "\n",
    "print(f\"Coherence Score c_v: {coherence_c_v}\")\n",
    "print(f\"Coherence Score u_mass: {coherence_u_mass}\")\n",
    "print(f\"Coherence Score npmi: {coherence_npmi}\")\n",
    "print(f\"Topic Diversity: {diversity_score}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_videos = defaultdict(list)\n",
    "\n",
    "video_topic_mapping_30 = {}\n",
    "\n",
    "# probability threshold for assigning multiple topics\n",
    "prob_threshold = 0.2\n",
    "\n",
    "# Dictionary to store topic words for each video\n",
    "video_topic_words_LDA1_30 = {}\n",
    "\n",
    "for idx, doc_bow in enumerate(corpus):\n",
    "    video_id = video_ids[idx]  # Get video ID\n",
    "    topic_distribution = lda_model_30.get_document_topics(doc_bow, minimum_probability=0)\n",
    "\n",
    "    # Get topics where probability is above threshold\n",
    "    assigned_topics = [topic for topic, prob in topic_distribution if prob >= prob_threshold]\n",
    "    video_topic_mapping_30[video_id] = assigned_topics  # Store assigned topics per video\n",
    "\n",
    "    for topic in assigned_topics:\n",
    "        topic_to_videos[topic].append(video_id)\n",
    "\n",
    "    # Get the representative words for each assigned topic\n",
    "    topic_words = []\n",
    "    for topic in assigned_topics:\n",
    "        words = [word for word, _ in lda_model_30.show_topic(topic, topn=100)]  # Get top 100 words\n",
    "        topic_words.append(\", \".join(words))  # Convert list to string\n",
    "\n",
    "    # Store the topic words as a string\n",
    "    video_topic_words_LDA1_30[video_id] = \"; \".join(topic_words)  \n",
    "\n",
    "# Count occurrences of each topic\n",
    "topic_counts_30 = Counter()\n",
    "\n",
    "for topics in video_topic_mapping_30.values():\n",
    "    for topic in topics:\n",
    "        topic_counts_30[topic] += 1\n",
    "\n",
    "# Print the number of videos per topic\n",
    "print(\"\\nNumber of Videos Per Topic:\")\n",
    "for topic, count in sorted(topic_counts_30.items()):\n",
    "    print(f\"Topic {topic}: {count} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the mapping of topic words to preprocessed text for each video\n",
    "video_topic_to_preprocessed_text = defaultdict(dict)\n",
    "\n",
    "# Iterate through the corpus and map topic words to preprocessed text\n",
    "for idx, doc_bow in enumerate(corpus):\n",
    "    video_id, preprocessed_tokens = filtered_documents_with_id[idx]  # Get video_id and preprocessed tokens\n",
    "    topic_distribution = lda_model_30.get_document_topics(doc_bow, minimum_probability=0)\n",
    "    assigned_topics = [topic for topic, prob in topic_distribution if prob >= prob_threshold]\n",
    "\n",
    "    for topic in assigned_topics:\n",
    "        # Get the top 100 words for the topic\n",
    "        topic_words = [word for word, _ in lda_model_30.show_topic(topic, topn=100)]\n",
    "        \n",
    "        # Find intersection between topic words and preprocessed tokens\n",
    "        overlapping_words = set(topic_words).intersection(set(preprocessed_tokens))\n",
    "        \n",
    "        # Store the overlapping words\n",
    "        video_topic_to_preprocessed_text[video_id][topic] = list(overlapping_words)\n",
    "\n",
    "# Now you can analyze the results\n",
    "for video_id, topic_mapping in video_topic_to_preprocessed_text.items():\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"\\nVideo ID: {video_id} | {video_title}\")\n",
    "    for topic, words in topic_mapping.items():\n",
    "        print(f\"  Topic {topic}: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the probability distribution of topics for each video\n",
    "video_topic_probabilities = {}\n",
    "\n",
    "# Iterate through the corpus and map topic words to preprocessed text\n",
    "for idx, doc_bow in enumerate(corpus):\n",
    "    video_id, preprocessed_tokens = filtered_documents_with_id[idx]  # Get video_id and preprocessed tokens\n",
    "    topic_distribution = lda_model_30.get_document_topics(doc_bow, minimum_probability=0)\n",
    "    assigned_topics = [topic for topic, prob in topic_distribution if prob >= prob_threshold]\n",
    "\n",
    "    # Store assigned topics for the video\n",
    "    video_topic_mapping_30[video_id] = assigned_topics\n",
    "\n",
    "    # Store the full topic probability distribution for the video\n",
    "    video_topic_probabilities[video_id] = topic_distribution\n",
    "\n",
    "    # Map videos to topics\n",
    "    for topic in assigned_topics:\n",
    "        topic_to_videos[topic].append(video_id)\n",
    "\n",
    "    # Get the representative words for each assigned topic\n",
    "    topic_words = []\n",
    "    for topic in assigned_topics:\n",
    "        words = [word for word, _ in lda_model_30.show_topic(topic, topn=100)]  # Get top 100 words\n",
    "        topic_words.append(\", \".join(words))  # Convert list to string\n",
    "\n",
    "    # Store the topic words as a string\n",
    "    video_topic_words_LDA1_30[video_id] = \"; \".join(topic_words)\n",
    "\n",
    "    # Map topic words to preprocessed text for the video\n",
    "    for topic in assigned_topics:\n",
    "        topic_words = [word for word, _ in lda_model_30.show_topic(topic, topn=100)]\n",
    "        overlapping_words = set(topic_words).intersection(set(preprocessed_tokens))\n",
    "        video_topic_to_preprocessed_text[video_id][topic] = list(overlapping_words)\n",
    "\n",
    "# Count occurrences of each topic\n",
    "topic_counts_30 = Counter()\n",
    "\n",
    "for video_id, topics in video_topic_mapping_30.items():\n",
    "    for topic in topics:\n",
    "        topic_counts_30[topic] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of videos per topic\n",
    "print(\"\\nNumber of Videos Per Topic:\")\n",
    "for topic, count in sorted(topic_counts_30.items()):\n",
    "    print(f\"Topic {topic}: {count} videos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mapping of topic words to preprocessed text for each video, including topic and word probabilities\n",
    "print(\"\\nMapping of Topic Words to Preprocessed Text (with Topic and Word Probabilities):\")\n",
    "for video_id, topic_mapping in video_topic_to_preprocessed_text.items():\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"\\nVideo ID: {video_id} | {video_title}\")\n",
    "    \n",
    "    # Get the topic probability distribution for the current video\n",
    "    topic_distribution = video_topic_probabilities[video_id]\n",
    "    \n",
    "    # Iterate through the assigned topics for the video\n",
    "    for topic, words in topic_mapping.items():\n",
    "        # Get the probability of the current topic for the video\n",
    "        topic_prob = next((prob for t, prob in topic_distribution if t == topic), 0.0)\n",
    "        \n",
    "        # Print the topic and its probability\n",
    "        print(f\"  Topic {topic} (Probability: {topic_prob:.4f}):\")\n",
    "        \n",
    "        # Get the word probabilities for the current topic\n",
    "        topic_words_with_probs = lda_model_30.show_topic(topic, topn=100)\n",
    "        \n",
    "        # Create a dictionary of word probabilities for easy lookup\n",
    "        word_prob_dict = {word: prob for word, prob in topic_words_with_probs}\n",
    "        \n",
    "        # Filter to include only the overlapping words and their probabilities\n",
    "        overlapping_words_with_probs = [(word, word_prob_dict[word]) for word in words if word in word_prob_dict]\n",
    "        \n",
    "        # Print the overlapping words and their probabilities\n",
    "        for word, prob in overlapping_words_with_probs:\n",
    "            print(f\"    - {word}: {prob:.4f}\")  # Print word and its probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print videos per topic\n",
    "print(\"\\nTop Words Per Topic:\")\n",
    "\n",
    "for topic_id in sorted(topic_to_videos.keys()): \n",
    "    top_words = lda_model_30.show_topic(topic_id, 50)\n",
    "    words_str = ', '.join([word for word, prob in top_words])\n",
    "    print(f\"Topic {topic_id}: {words_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the videos under each topic in the desired format\n",
    "print(\"\\nVideos Under Each Topic:\")\n",
    "for topic, videos in sorted(topic_to_videos.items()):\n",
    "    # Remove duplicate video IDs\n",
    "    unique_videos = list(set(videos))  \n",
    "\n",
    "    # Get the number of unique videos for the current topic\n",
    "    num_videos = len(unique_videos)\n",
    "    print(f\"Topic {topic} ({num_videos} videos):\")\n",
    "\n",
    "    for video_id in unique_videos:\n",
    "        video_title = video_titles.get(video_id, \"Unknown Title\")  # Get the title or default to \"Unknown Title\"\n",
    "        print(f\"  - {video_id} | {video_title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the probability distribution of topics for each video\n",
    "print(\"\\nProbability Distribution of Topics Per Video:\")\n",
    "for video_id, topic_distribution in video_topic_probabilities.items():\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")  # Get the title or default to \"Unknown Title\"\n",
    "    print(f\"Video ID: {video_id} | {video_title}\")\n",
    "    for topic, prob in sorted(topic_distribution, key=lambda x: x[0]):  # Sort by topic number\n",
    "        print(f\"  - Topic {topic}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mapping of topic words to preprocessed text for each video\n",
    "print(\"\\nMapping of Topic Words to Preprocessed Text:\")\n",
    "for video_id, topic_mapping in video_topic_to_preprocessed_text.items():\n",
    "    print(f\"Video ID: {video_id}\")\n",
    "    for topic, words in topic_mapping.items():\n",
    "        print(f\"  Topic {topic}: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top words per topic with probabilities\n",
    "print(\"\\nTop Words Per Topic:\")\n",
    "\n",
    "for topic_id in sorted(topic_to_videos.keys()): \n",
    "    top_words = lda_model_30.show_topic(topic_id, 50)  # Get top 50 words with probabilities\n",
    "    words_str = ', '.join([f\"{word} ({prob:.4f})\" for word, prob in top_words])\n",
    "    print(f\"Topic {topic_id}: {words_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "print(\"\\nVideos Assigned Per Topic:\")\n",
    "for topic, video_list in sorted(topic_to_videos.items()):\n",
    "    print(f\"\\nTopic {topic}:\")\n",
    "\n",
    "    # Select up to 5 random videos for this topic\n",
    "    random_videos = random.sample(video_list, min(10, len(video_list)))\n",
    "\n",
    "    for video in random_videos:\n",
    "        video_title = video_titles.get(video, \"Unknown Title\")\n",
    "        print(f\"{video} | {video_title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 random videos overall\n",
    "random_video_ids = random.sample(list(video_topic_mapping_30.keys()), min(10, len(video_topic_mapping_30)))\n",
    "\n",
    "print(\"\\nTopics Assigned Per 10 Random Videos:\")\n",
    "for video_id in random_video_ids:\n",
    "    topic_list = ', '.join(map(str, video_topic_mapping_30[video_id])) if video_topic_mapping_30[video_id] else \"No dominant topic\"\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"Video ID: {video_id} | {video_title} → Topics: {topic_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 random videos for word contributions\n",
    "random_contrib_videos = random.sample(list(topic_word_contributions_30.keys()), min(10, len(topic_word_contributions_30)))\n",
    "\n",
    "print(\"\\nWords Contributing to Topics Per 10 Random Videos:\")\n",
    "for video_id in random_contrib_videos:\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"\\nVideo ID: {video_id} | {video_title}\")\n",
    "\n",
    "    for topic, word_scores in topic_word_contributions_30[video_id].items():\n",
    "        # Ensure word_scores is a list of tuples\n",
    "        if isinstance(word_scores, list):\n",
    "            sorted_words = sorted(word_scores, key=lambda x: x[1], reverse=True)  # Sort tuples\n",
    "            top_contributing_words = \", \".join([f\"{word} ({round(score, 3)})\" for word, score in sorted_words])\n",
    "        else:\n",
    "            top_contributing_words = \"No words available\"\n",
    "\n",
    "        topic_prob = dict(video_topic_mapping_30[video_id]).get(topic, 0)  # Get probability safely\n",
    "\n",
    "        if top_contributing_words.strip():\n",
    "            print(f\"  Topic {topic} (Probability: {round(topic_prob, 3)}):\\n    Contributing Words: {top_contributing_words}\\n\")\n",
    "        else:\n",
    "            print(f\"  Topic {topic} (Probability: {round(topic_prob, 3)}):\\n    No detected contributing words!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_25 = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=25,\n",
    "        alpha='asymmetric',\n",
    "        eta=0.1,\n",
    "        passes=50,  \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Compute Coherence Scores\n",
    "coherence_c_v = CoherenceModel(model=lda_model_25, texts=filtered_documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "coherence_u_mass = CoherenceModel(model=lda_model_25, corpus=corpus, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "coherence_npmi = CoherenceModel(model=lda_model_25, texts=filtered_documents, dictionary=dictionary, coherence='c_npmi').get_coherence()\n",
    "\n",
    "# Compute Topic Diversity\n",
    "diversity_score = topic_diversity(lda_model_25)\n",
    "\n",
    "# Compute Jaccard Similarity\n",
    "jaccard_score = avg_jaccard_similarity(lda_model_25)\n",
    "\n",
    "print(f\"Coherence Score c_v: {coherence_c_v}\")\n",
    "print(f\"Coherence Score u_mass: {coherence_u_mass}\")\n",
    "print(f\"Coherence Score npmi: {coherence_npmi}\")\n",
    "print(f\"Topic Diversity: {diversity_score}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_score}\")\n",
    "\n",
    "topic_to_videos_25 = defaultdict(list)\n",
    "\n",
    "video_topic_mapping_25 = {}\n",
    "\n",
    "# probability threshold for assigning multiple topics\n",
    "prob_threshold = 0.2\n",
    "\n",
    "# Dictionary to store topic words for each video\n",
    "video_topic_words_LDA1_25 = {}\n",
    "\n",
    "for idx, doc_bow in enumerate(corpus):\n",
    "    video_id = video_ids[idx]  # Get video ID\n",
    "    topic_distribution = lda_model_25.get_document_topics(doc_bow, minimum_probability=0)\n",
    "\n",
    "    # Get topics where probability is above threshold\n",
    "    assigned_topics = [topic for topic, prob in topic_distribution if prob >= prob_threshold]\n",
    "    video_topic_mapping_25[video_id] = assigned_topics  # Store assigned topics per video\n",
    "\n",
    "    for topic in assigned_topics:\n",
    "        topic_to_videos_25[topic].append(video_id)\n",
    "\n",
    "    # Get the representative words for each assigned topic\n",
    "    topic_words = []\n",
    "    for topic in assigned_topics:\n",
    "        words = [word for word, _ in lda_model_25.show_topic(topic, topn=50)]  # Get top 100 words\n",
    "        topic_words.append(\", \".join(words))  # Convert list to string\n",
    "\n",
    "# Count occurrences of each topic\n",
    "topic_counts_25 = Counter()\n",
    "\n",
    "for topics in video_topic_mapping_25.values():\n",
    "    for topic in topics:\n",
    "        topic_counts_25[topic] += 1\n",
    "\n",
    "# Print videos per topic\n",
    "print(\"\\nTop Words Per Topic:\")\n",
    "\n",
    "for topic_id in sorted(topic_to_videos_25.keys()): \n",
    "    top_words = lda_model_25.show_topic(topic_id, 50)\n",
    "    words_str = ', '.join([word for word, prob in top_words])\n",
    "    print(f\"Topic {topic_id}: {words_str}\")\n",
    "    \n",
    "# Print the number of videos per topic\n",
    "print(\"\\nNumber of Videos Per Topic:\")\n",
    "for topic, count in sorted(topic_counts_25.items()):\n",
    "    print(f\"Topic {topic}: {count} videos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTopics Assigned Per Video:\")\n",
    "for video_id, topics in video_topic_mapping_25.items():\n",
    "    topic_list = ', '.join(map(str, topics)) if topics else \"No dominant topic\"\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"Video ID: {video_id} | {video_title} → Topics: {topic_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVideos Assigned Per Topic:\")\n",
    "for topic, video_list in sorted(topic_to_videos_25.items()):\n",
    "    print(f\"\\nTopic {topic}:\")\n",
    "    for video in video_list:\n",
    "        video_title = video_titles.get(video, \"Unknown Title\")\n",
    "        print(f\"\\n{video} | {video_title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_videos_25 = defaultdict(list)\n",
    "video_topic_mapping_25 = {}\n",
    "prob_threshold = 0.2\n",
    "video_topic_words_LDA1_25 = defaultdict(list)  # Store topics separately\n",
    "topic_word_contributions_25 = {}\n",
    "\n",
    "for idx, doc_bow in enumerate(corpus):\n",
    "    video_id = video_ids[idx]  # Get video ID\n",
    "    topic_distribution = lda_model_25.get_document_topics(doc_bow, minimum_probability=0)\n",
    "\n",
    "    # Get topics above threshold\n",
    "    assigned_topics = {topic: prob for topic, prob in topic_distribution if prob >= prob_threshold}\n",
    "    video_topic_mapping_25[video_id] = assigned_topics\n",
    "\n",
    "    for topic in assigned_topics:\n",
    "        topic_to_videos_25[topic].append(video_id)\n",
    "\n",
    "    # Identify words contributing to each assigned topic\n",
    "    topic_word_contributions_25[video_id] = defaultdict(dict)\n",
    "    bow_words = {dictionary[word_id] for word_id, _ in doc_bow}  # Convert doc_bow IDs to words\n",
    "\n",
    "    for topic, prob in assigned_topics.items():\n",
    "        topic_top_words = dict(lda_model_25.show_topic(topic, topn=2000))  # Get top words for the topic\n",
    "        word_contributions = {}\n",
    "\n",
    "        # Ensure we capture all contributing words\n",
    "        for word in bow_words:\n",
    "            if word in topic_top_words:\n",
    "                word_contributions[word] = topic_top_words[word]\n",
    "\n",
    "        # Store word contributions per topic\n",
    "        topic_word_contributions_25[video_id][topic] = word_contributions\n",
    "        topic_words = \", \".join(topic_top_words.keys())\n",
    "        video_topic_words_LDA1_25[video_id].append(f\"Topic {topic}: {topic_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print contributing words per video\n",
    "print(\"\\nWords Contributing to Topics Per Video:\")\n",
    "for video_id, topic_data in topic_word_contributions_25.items():\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"\\nVideo ID: {video_id} | {video_title}\")\n",
    "    for topic, word_scores in topic_data.items():\n",
    "        sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_contributing_words = \", \".join([f\"{word} ({round(score, 3)})\" for word, score in sorted_words])\n",
    "        topic_prob = video_topic_mapping_25[video_id][topic]\n",
    "        \n",
    "        # Ensure that all contributing words are displayed\n",
    "        if top_contributing_words.strip():\n",
    "            print(f\"  Topic {topic} (Probability: {round(topic_prob, 3)}):\\n    Contributing Words: {top_contributing_words}\\n\")\n",
    "        else:\n",
    "            print(f\"  Topic {topic} (Probability: {round(topic_prob, 3)}):\\n    No detected contributing words!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVideos Assigned Per Topic:\")\n",
    "for topic, video_list in sorted(topic_to_videos_25.items()):\n",
    "    print(f\"\\nTopic {topic}:\")\n",
    "\n",
    "    # Select up to 10 random videos for this topic\n",
    "    random_videos = random.sample(video_list, min(10, len(video_list)))\n",
    "\n",
    "    for video in random_videos:\n",
    "        video_title = video_titles.get(video, \"Unknown Title\")\n",
    "        print(f\"{video} | {video_title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 random videos overall\n",
    "random_video_ids = random.sample(list(video_topic_mapping_25.keys()), min(10, len(video_topic_mapping_25)))\n",
    "\n",
    "print(\"\\nTopics Assigned Per 10 Random Videos:\")\n",
    "for video_id in random_video_ids:\n",
    "    topic_list = ', '.join(map(str, video_topic_mapping_25[video_id])) if video_topic_mapping_25[video_id] else \"No dominant topic\"\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"Video ID: {video_id} | {video_title} → Topics: {topic_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 random videos for word contributions\n",
    "random_contrib_videos = random.sample(list(topic_word_contributions_25.keys()), min(10, len(topic_word_contributions_25)))\n",
    "\n",
    "print(\"\\nWords Contributing to Topics Per 10 Random Videos:\")\n",
    "for video_id in random_contrib_videos:\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"\\nVideo ID: {video_id} | {video_title}\")\n",
    "    \n",
    "    for topic, word_scores in topic_word_contributions_25[video_id].items():\n",
    "        sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_contributing_words = \", \".join([f\"{word} ({round(score, 3)})\" for word, score in sorted_words])\n",
    "        topic_prob = video_topic_mapping_25[video_id][topic]\n",
    "\n",
    "        if top_contributing_words.strip():\n",
    "            print(f\"  Topic {topic} (Probability: {round(topic_prob, 3)}):\\n    Contributing Words: {top_contributing_words}\\n\")\n",
    "        else:\n",
    "            print(f\"  Topic {topic} (Probability: {round(topic_prob, 3)}):\\n    No detected contributing words!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model_20 = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=20,\n",
    "        alpha=0.5,\n",
    "        eta=0.01,\n",
    "        passes=50,  \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Compute Coherence Scores\n",
    "coherence_c_v = CoherenceModel(model=lda_model_20, texts=filtered_documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "coherence_u_mass = CoherenceModel(model=lda_model_20, corpus=corpus, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "coherence_npmi = CoherenceModel(model=lda_model_20, texts=filtered_documents, dictionary=dictionary, coherence='c_npmi').get_coherence()\n",
    "\n",
    "# Compute Topic Diversity\n",
    "diversity_score = topic_diversity(lda_model_20)\n",
    "\n",
    "# Compute Jaccard Similarity\n",
    "jaccard_score = avg_jaccard_similarity(lda_model_20)\n",
    "\n",
    "print(f\"Coherence Score c_v: {coherence_c_v}\")\n",
    "print(f\"Coherence Score u_mass: {coherence_u_mass}\")\n",
    "print(f\"Coherence Score npmi: {coherence_npmi}\")\n",
    "print(f\"Topic Diversity: {diversity_score}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_score}\")\n",
    "\n",
    "topic_to_videos_20 = defaultdict(list)\n",
    "\n",
    "video_topic_mapping_20 = {}\n",
    "\n",
    "# probability threshold for assigning multiple topics\n",
    "prob_threshold = 0.2\n",
    "\n",
    "# Dictionary to store topic words for each video\n",
    "video_topic_words_LDA1_20 = {}\n",
    "\n",
    "for idx, doc_bow in enumerate(corpus):\n",
    "    video_id = video_ids[idx]  # Get video ID\n",
    "    topic_distribution = lda_model_20.get_document_topics(doc_bow, minimum_probability=0)\n",
    "\n",
    "    # Get topics where probability is above threshold\n",
    "    assigned_topics = [topic for topic, prob in topic_distribution if prob >= prob_threshold]\n",
    "    video_topic_mapping_20[video_id] = assigned_topics  # Store assigned topics per video\n",
    "\n",
    "    for topic in assigned_topics:\n",
    "        topic_to_videos_20[topic].append(video_id)\n",
    "\n",
    "    # Get the representative words for each assigned topic\n",
    "    topic_words = []\n",
    "    for topic in assigned_topics:\n",
    "        words = [word for word, _ in lda_model_20.show_topic(topic, topn=50)]  # Get top 100 words\n",
    "        topic_words.append(\", \".join(words))  # Convert list to string\n",
    "\n",
    "# Count occurrences of each topic\n",
    "topic_counts_20 = Counter()\n",
    "\n",
    "for topics in video_topic_mapping_20.values():\n",
    "    for topic in topics:\n",
    "        topic_counts_20[topic] += 1\n",
    "\n",
    "# Print videos per topic\n",
    "print(\"\\nTop Words Per Topic:\")\n",
    "\n",
    "for topic_id in sorted(topic_to_videos_20.keys()): \n",
    "    top_words = lda_model_25.show_topic(topic_id, 50)\n",
    "    words_str = ', '.join([word for word, prob in top_words])\n",
    "    print(f\"Topic {topic_id}: {words_str}\")\n",
    "    \n",
    "# Print the number of videos per topic\n",
    "print(\"\\nNumber of Videos Per Topic:\")\n",
    "for topic, count in sorted(topic_counts_20.items()):\n",
    "    print(f\"Topic {topic}: {count} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTopics Assigned Per Video:\")\n",
    "for video_id, topics in video_topic_mapping_20.items():\n",
    "    topic_list = ', '.join(map(str, topics)) if topics else \"No dominant topic\"\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"Video ID: {video_id} | {video_title} → Topics: {topic_list}\")\n",
    "\n",
    "print(\"\\nVideos Assigned Per Topic:\")\n",
    "for topic, video_list in sorted(topic_to_videos_20.items()):\n",
    "    print(f\"\\nTopic {topic}:\")\n",
    "    for video in video_list:\n",
    "        video_title = video_titles.get(video, \"Unknown Title\")\n",
    "        print(f\"\\n{video} | {video_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_videos_20 = defaultdict(list)\n",
    "video_topic_mapping_20 = {}\n",
    "prob_threshold = 0.2\n",
    "video_topic_words_LDA1_20 = defaultdict(list)  # Store topics separately\n",
    "topic_word_contributions_20 = {}\n",
    "\n",
    "for idx, doc_bow in enumerate(corpus):\n",
    "    video_id = video_ids[idx]  # Get video ID\n",
    "    topic_distribution = lda_model_20.get_document_topics(doc_bow, minimum_probability=0)\n",
    "\n",
    "    # Get topics above threshold\n",
    "    assigned_topics = {topic: prob for topic, prob in topic_distribution if prob >= prob_threshold}\n",
    "    video_topic_mapping_20[video_id] = assigned_topics\n",
    "\n",
    "    for topic in assigned_topics:\n",
    "        topic_to_videos_20[topic].append(video_id)\n",
    "\n",
    "    # Identify words contributing to each assigned topic\n",
    "    topic_word_contributions_20[video_id] = defaultdict(dict)\n",
    "    bow_words = {dictionary[word_id] for word_id, _ in doc_bow}  # Convert doc_bow IDs to words\n",
    "\n",
    "    for topic, prob in assigned_topics.items():\n",
    "        topic_top_words = dict(lda_model_20.show_topic(topic, topn=2000))  # Get top words for the topic\n",
    "        word_contributions = {}\n",
    "\n",
    "        # Ensure we capture all contributing words\n",
    "        for word in bow_words:\n",
    "            if word in topic_top_words:\n",
    "                word_contributions[word] = topic_top_words[word]\n",
    "\n",
    "        # Store word contributions per topic\n",
    "        topic_word_contributions_20[video_id][topic] = word_contributions\n",
    "        topic_words = \", \".join(topic_top_words.keys())\n",
    "        video_topic_words_LDA1_20[video_id].append(f\"Topic {topic}: {topic_words}\")\n",
    "\n",
    "# Print contributing words per video\n",
    "print(\"\\nWords Contributing to Topics Per Video:\")\n",
    "for video_id, topic_data in topic_word_contributions_20.items():\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"\\nVideo ID: {video_id} | {video_title}\")\n",
    "    for topic, word_scores in topic_data.items():\n",
    "        sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_contributing_words = \", \".join([f\"{word} ({round(score, 3)})\" for word, score in sorted_words])\n",
    "        topic_prob = video_topic_mapping_20[video_id][topic]\n",
    "        \n",
    "        # Ensure that all contributing words are displayed\n",
    "        if top_contributing_words.strip():\n",
    "            print(f\"  Topic {topic} (Probability: {round(topic_prob, 3)}):\\n    Contributing Words: {top_contributing_words}\\n\")\n",
    "        else:\n",
    "            print(f\"  Topic {topic} (Probability: {round(topic_prob, 3)}):\\n    No detected contributing words!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model_15 = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=15,\n",
    "        alpha=0.5,\n",
    "        eta=0.01,\n",
    "        passes=50,  \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Compute Coherence Scores\n",
    "coherence_c_v = CoherenceModel(model=lda_model_15, texts=filtered_documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "coherence_u_mass = CoherenceModel(model=lda_model_15, corpus=corpus, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "coherence_npmi = CoherenceModel(model=lda_model_15, texts=filtered_documents, dictionary=dictionary, coherence='c_npmi').get_coherence()\n",
    "\n",
    "# Compute Topic Diversity\n",
    "diversity_score = topic_diversity(lda_model_15)\n",
    "\n",
    "# Compute Jaccard Similarity\n",
    "jaccard_score = avg_jaccard_similarity(lda_model_15)\n",
    "\n",
    "print(f\"Coherence Score c_v: {coherence_c_v}\")\n",
    "print(f\"Coherence Score u_mass: {coherence_u_mass}\")\n",
    "print(f\"Coherence Score npmi: {coherence_npmi}\")\n",
    "print(f\"Topic Diversity: {diversity_score}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_score}\")\n",
    "\n",
    "topic_to_videos_15 = defaultdict(list)\n",
    "\n",
    "video_topic_mapping_15 = {}\n",
    "\n",
    "# probability threshold for assigning multiple topics\n",
    "prob_threshold = 0.2\n",
    "\n",
    "# Dictionary to store topic words for each video\n",
    "video_topic_words_LDA1_15 = {}\n",
    "\n",
    "for idx, doc_bow in enumerate(corpus):\n",
    "    video_id = video_ids[idx]  # Get video ID\n",
    "    topic_distribution = lda_model_15.get_document_topics(doc_bow, minimum_probability=0)\n",
    "\n",
    "    # Get topics where probability is above threshold\n",
    "    assigned_topics = [topic for topic, prob in topic_distribution if prob >= prob_threshold]\n",
    "    video_topic_mapping_15[video_id] = assigned_topics  # Store assigned topics per video\n",
    "\n",
    "    for topic in assigned_topics:\n",
    "        topic_to_videos_15[topic].append(video_id)\n",
    "\n",
    "    # Get the representative words for each assigned topic\n",
    "    topic_words = []\n",
    "    for topic in assigned_topics:\n",
    "        words = [word for word, _ in lda_model_15.show_topic(topic, topn=50)]  # Get top 100 words\n",
    "        topic_words.append(\", \".join(words))  # Convert list to string\n",
    "\n",
    "# Count occurrences of each topic\n",
    "topic_counts_15 = Counter()\n",
    "\n",
    "for topics in video_topic_mapping_15.values():\n",
    "    for topic in topics:\n",
    "        topic_counts_15[topic] += 1\n",
    "\n",
    "# Print videos per topic\n",
    "print(\"\\nTop Words Per Topic:\")\n",
    "\n",
    "for topic_id in sorted(topic_to_videos_15.keys()): \n",
    "    top_words = lda_model_15.show_topic(topic_id, 50)\n",
    "    words_str = ', '.join([word for word, prob in top_words])\n",
    "    print(f\"Topic {topic_id}: {words_str}\")\n",
    "    \n",
    "# Print the number of videos per topic\n",
    "print(\"\\nNumber of Videos Per Topic:\")\n",
    "for topic, count in sorted(topic_counts_15.items()):\n",
    "    print(f\"Topic {topic}: {count} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTopics Assigned Per Video:\")\n",
    "for video_id, topics in video_topic_mapping_15.items():\n",
    "    topic_list = ', '.join(map(str, topics)) if topics else \"No dominant topic\"\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"Video ID: {video_id} | {video_title} → Topics: {topic_list}\")\n",
    "\n",
    "print(\"\\nVideos Assigned Per Topic:\")\n",
    "for topic, video_list in sorted(topic_to_videos_15.items()):\n",
    "    print(f\"\\nTopic {topic}:\")\n",
    "    for video in video_list:\n",
    "        video_title = video_titles.get(video, \"Unknown Title\")\n",
    "        print(f\"\\n{video} | {video_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_videos_15 = defaultdict(list)\n",
    "video_topic_mapping_15 = {}\n",
    "prob_threshold = 0.2\n",
    "video_topic_words_LDA1_15 = defaultdict(list)  # Store topics separately\n",
    "topic_word_contributions_15 = {}\n",
    "\n",
    "for idx, doc_bow in enumerate(corpus):\n",
    "    video_id = video_ids[idx]  # Get video ID\n",
    "    topic_distribution = lda_model_15.get_document_topics(doc_bow, minimum_probability=0)\n",
    "\n",
    "    # Get topics above threshold\n",
    "    assigned_topics = {topic: prob for topic, prob in topic_distribution if prob >= prob_threshold}\n",
    "    video_topic_mapping_15[video_id] = assigned_topics\n",
    "\n",
    "    for topic in assigned_topics:\n",
    "        topic_to_videos_15[topic].append(video_id)\n",
    "\n",
    "    # Identify words contributing to each assigned topic\n",
    "    topic_word_contributions_15[video_id] = defaultdict(dict)\n",
    "    bow_words = {dictionary[word_id] for word_id, _ in doc_bow}  # Convert doc_bow IDs to words\n",
    "\n",
    "    for topic, prob in assigned_topics.items():\n",
    "        topic_top_words = dict(lda_model_15.show_topic(topic, topn=2000))  # Get top words for the topic\n",
    "        word_contributions = {}\n",
    "\n",
    "        # Ensure we capture all contributing words\n",
    "        for word in bow_words:\n",
    "            if word in topic_top_words:\n",
    "                word_contributions[word] = topic_top_words[word]\n",
    "\n",
    "        # Store word contributions per topic\n",
    "        topic_word_contributions_15[video_id][topic] = word_contributions\n",
    "        topic_words = \", \".join(topic_top_words.keys())\n",
    "        video_topic_words_LDA1_15[video_id].append(f\"Topic {topic}: {topic_words}\")\n",
    "\n",
    "# Print contributing words per video\n",
    "print(\"\\nWords Contributing to Topics Per Video:\")\n",
    "for video_id, topic_data in topic_word_contributions_15.items():\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"\\nVideo ID: {video_id} | {video_title}\")\n",
    "    for topic, word_scores in topic_data.items():\n",
    "        sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_contributing_words = \", \".join([f\"{word} ({round(score, 3)})\" for word, score in sorted_words])\n",
    "        topic_prob = video_topic_mapping_15[video_id][topic]\n",
    "        \n",
    "        # Ensure that all contributing words are displayed\n",
    "        if top_contributing_words.strip():\n",
    "            print(f\"  Topic {topic} (Probability: {round(topic_prob, 3)}):\\n    Contributing Words: {top_contributing_words}\\n\")\n",
    "        else:\n",
    "            print(f\"  Topic {topic} (Probability: {round(topic_prob, 3)}):\\n    No detected contributing words!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_10 = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=10,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        passes=50,  \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Compute Coherence Scores\n",
    "coherence_c_v = CoherenceModel(model=lda_model_10, texts=filtered_documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "coherence_u_mass = CoherenceModel(model=lda_model_10, corpus=corpus, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "coherence_npmi = CoherenceModel(model=lda_model_10, texts=filtered_documents, dictionary=dictionary, coherence='c_npmi').get_coherence()\n",
    "\n",
    "# Compute Topic Diversity\n",
    "diversity_score = topic_diversity(lda_model_10)\n",
    "\n",
    "# Compute Jaccard Similarity\n",
    "jaccard_score = avg_jaccard_similarity(lda_model_10)\n",
    "\n",
    "print(f\"Coherence Score c_v: {coherence_c_v}\")\n",
    "print(f\"Coherence Score u_mass: {coherence_u_mass}\")\n",
    "print(f\"Coherence Score npmi: {coherence_npmi}\")\n",
    "print(f\"Topic Diversity: {diversity_score}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_score}\")\n",
    "\n",
    "topic_to_videos_10 = defaultdict(list)\n",
    "\n",
    "video_topic_mapping_10 = {}\n",
    "\n",
    "# probability threshold for assigning multiple topics\n",
    "prob_threshold = 0.2\n",
    "\n",
    "# Dictionary to store topic words for each video\n",
    "video_topic_words_LDA1_10 = {}\n",
    "\n",
    "for idx, doc_bow in enumerate(corpus):\n",
    "    video_id = video_ids[idx]  # Get video ID\n",
    "    topic_distribution = lda_model_10.get_document_topics(doc_bow, minimum_probability=0)\n",
    "\n",
    "    # Get topics where probability is above threshold\n",
    "    assigned_topics = [topic for topic, prob in topic_distribution if prob >= prob_threshold]\n",
    "    video_topic_mapping_10[video_id] = assigned_topics  # Store assigned topics per video\n",
    "\n",
    "    for topic in assigned_topics:\n",
    "        topic_to_videos_10[topic].append(video_id)\n",
    "\n",
    "    # Get the representative words for each assigned topic\n",
    "    topic_words = []\n",
    "    for topic in assigned_topics:\n",
    "        words = [word for word, _ in lda_model_10.show_topic(topic, topn=50)]  # Get top 100 words\n",
    "        topic_words.append(\", \".join(words))  # Convert list to string\n",
    "\n",
    "# Count occurrences of each topic\n",
    "topic_counts_10 = Counter()\n",
    "\n",
    "for topics in video_topic_mapping_10.values():\n",
    "    for topic in topics:\n",
    "        topic_counts_10[topic] += 1\n",
    "\n",
    "# Print videos per topic\n",
    "print(\"\\nTop Words Per Topic:\")\n",
    "\n",
    "for topic_id in sorted(topic_to_videos_10.keys()): \n",
    "    top_words = lda_model_10.show_topic(topic_id, 50)\n",
    "    words_str = ', '.join([word for word, prob in top_words])\n",
    "    print(f\"Topic {topic_id}: {words_str}\")\n",
    "    \n",
    "# Print the number of videos per topic\n",
    "print(\"\\nNumber of Videos Per Topic:\")\n",
    "for topic, count in sorted(topic_counts_10.items()):\n",
    "    print(f\"Topic {topic}: {count} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTopics Assigned Per Video:\")\n",
    "for video_id, topics in video_topic_mapping_10.items():\n",
    "    topic_list = ', '.join(map(str, topics)) if topics else \"No dominant topic\"\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"Video ID: {video_id} | {video_title} → Topics: {topic_list}\")\n",
    "\n",
    "print(\"\\nVideos Assigned Per Topic:\")\n",
    "for topic, video_list in sorted(topic_to_videos_10.items()):\n",
    "    print(f\"\\nTopic {topic}:\")\n",
    "    for video in video_list:\n",
    "        video_title = video_titles.get(video, \"Unknown Title\")\n",
    "        print(f\"\\n{video} | {video_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_videos_10 = defaultdict(list)\n",
    "video_topic_mapping_10 = {}\n",
    "prob_threshold = 0.2\n",
    "video_topic_words_LDA1_10 = defaultdict(list)  # Store topics separately\n",
    "topic_word_contributions_10 = {}\n",
    "\n",
    "for idx, doc_bow in enumerate(corpus):\n",
    "    video_id = video_ids[idx]  # Get video ID\n",
    "    topic_distribution = lda_model_10.get_document_topics(doc_bow, minimum_probability=0)\n",
    "\n",
    "    # Get topics above threshold\n",
    "    assigned_topics = {topic: prob for topic, prob in topic_distribution if prob >= prob_threshold}\n",
    "    video_topic_mapping_10[video_id] = assigned_topics\n",
    "\n",
    "    for topic in assigned_topics:\n",
    "        topic_to_videos_10[topic].append(video_id)\n",
    "\n",
    "    # Identify words contributing to each assigned topic\n",
    "    topic_word_contributions_10[video_id] = defaultdict(dict)\n",
    "    bow_words = {dictionary[word_id] for word_id, _ in doc_bow}  # Convert doc_bow IDs to words\n",
    "\n",
    "    for topic, prob in assigned_topics.items():\n",
    "        topic_top_words = dict(lda_model_10.show_topic(topic, topn=2000))  # Get top words for the topic\n",
    "        word_contributions = {}\n",
    "\n",
    "        # Ensure we capture all contributing words\n",
    "        for word in bow_words:\n",
    "            if word in topic_top_words:\n",
    "                word_contributions[word] = topic_top_words[word]\n",
    "\n",
    "        # Store word contributions per topic\n",
    "        topic_word_contributions_10[video_id][topic] = word_contributions\n",
    "        topic_words = \", \".join(topic_top_words.keys())\n",
    "        video_topic_words_LDA1_15[video_id].append(f\"Topic {topic}: {topic_words}\")\n",
    "\n",
    "# Print contributing words per video\n",
    "print(\"\\nWords Contributing to Topics Per Video:\")\n",
    "for video_id, topic_data in topic_word_contributions_10.items():\n",
    "    video_title = video_titles.get(video_id, \"Unknown Title\")\n",
    "    print(f\"\\nVideo ID: {video_id} | {video_title}\")\n",
    "    for topic, word_scores in topic_data.items():\n",
    "        sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_contributing_words = \", \".join([f\"{word} ({round(score, 3)})\" for word, score in sorted_words])\n",
    "        topic_prob = video_topic_mapping_10[video_id][topic]\n",
    "        \n",
    "        # Ensure that all contributing words are displayed\n",
    "        if top_contributing_words.strip():\n",
    "            print(f\"  Topic {topic} (Probability: {round(topic_prob, 3)}):\\n    Contributing Words: {top_contributing_words}\\n\")\n",
    "        else:\n",
    "            print(f\"  Topic {topic} (Probability: {round(topic_prob, 3)}):\\n    No detected contributing words!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
