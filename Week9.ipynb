{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import re\n",
    "import nltk\n",
    "import requests\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "# Make sure to download nltk stopwords if not already\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to transcripts and tags directories\n",
    "transcripts_dir = 'cleaned_transcripts/'\n",
    "tags_dir = 'tags/'\n",
    "csv_path = 'cleaned_results.xlsx'\n",
    "\n",
    "# Load the CSV file and filter for \"related\" videos\n",
    "csv_data = pd.read_excel(csv_path)\n",
    "related_videos = csv_data[csv_data[\"related\"] == \"yes\"]\n",
    "\n",
    "# Load transcripts\n",
    "transcripts = []\n",
    "tags = []\n",
    "\n",
    "# Process each related video based on its video_id\n",
    "for video_id in related_videos[\"Video Id\"]:\n",
    "    # Construct paths based on video ID naming conventions\n",
    "    transcript_file = os.path.join(transcripts_dir, f\"{video_id}_captions.txt\")\n",
    "    tag_file = os.path.join(tags_dir, f\"{video_id}.txt\")\n",
    "    \n",
    "    # Read the transcript and tag files if they exist\n",
    "    try:\n",
    "        with open(transcript_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            transcripts.append(file.read())\n",
    "        with open(tag_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            tags.append(file.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Files for video ID {video_id} not found, skipping.\")\n",
    "\n",
    "# Combine the filtered data into a DataFrame\n",
    "data = pd.DataFrame({\"video_id\": related_videos[\"Video Id\"], \"tags\": tags, \"transcripts\": transcripts})\n",
    "data[\"text\"] = data[\"tags\"] + \" \" + data[\"transcripts\"]\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Text Preprocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Function to fetch stopwords from GitHub URL\n",
    "def fetch_stopwords_from_github(url):\n",
    "    response = requests.get(url)\n",
    "    github_stopwords = response.text.splitlines()  # Split by new lines\n",
    "    return set(github_stopwords)\n",
    "\n",
    "# GitHub URL for stopwords\n",
    "github_stopwords_url = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt'\n",
    "github_stopwords = fetch_stopwords_from_github(github_stopwords_url)\n",
    "\n",
    "\n",
    "custom_stop_words = ['like', 'yeah', 'know', 'um', 'uh', 'really', 'one', 'go', 'right', 'okay', 'well', 'said', \n",
    "                     'going', 'got', 'na', 'always', 'every', 'each', 'say', 'el', 'little', 'still', \n",
    "                     'best', 'dutch', 'nice', 'great', 'awesome', 'good', 'cool', 'love', 'amazing', 'wow' ]\n",
    "broad_terms = ['philippines', 'philippine', 'british', 'filipino', 'video', 'http', 'korea', 'korean', \n",
    "               'youtube', 'google', 'united', 'america', 'american']\n",
    "kpop_keywords = ['kpop', '필리핀', 'bts', 'blackpink', 'twice', 'exo', 'k-pop', 'seventeen', \n",
    "                 'stray kids', 'nct', 'kdrama', 'aespa', 'taehyung', 'jimin', 'jungkook']\n",
    "more_keywords = [\n",
    "    'breaking news', 'report', 'coverage', 'investigation', 'interview', 'documentary', \n",
    "    'journalist', 'headline', 'reporter', 'current events', 'special report', \n",
    "    'analysis', 'documented', 'broadcast', 'reporting', 'v', 'food', 'travel', 'react', \n",
    "    'reacts', 'reaction', 'foreigner', 'thing', 'visit', 'dc', 'japan', 'first', 'fast', \n",
    "    'asia', 'ang', 'indian', 'thai', 'vietnamese', 'russia', 'gon', 'canada', 'canadian', 'russian', \n",
    "    'russia', 'guy', 'lot', 'bit', 'diba', 'ola', 'cuz', 'thai', 'thailand', 'person', 'citizen', 'foreigner', 'foreign', 'foreigners',\n",
    "    'facebook', 'filipinos', 'filipinas', 'vlog', 'vlogs', 'vlogging', 'hashtag', 'india', 'bro', 'dito', 'people', 'time', 'music', 'guys'\n",
    "]\n",
    "\n",
    "# Add custom and broad terms\n",
    "stop_words.update(custom_stop_words, broad_terms, kpop_keywords, more_keywords, github_stopwords)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and remove non-alphabet characters\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords and filter out short words\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 1]\n",
    "    return words\n",
    "\n",
    "data[\"cleaned_text\"] = data[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Vectorize text with TF-IDF to remove low-impact words\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, min_df=5)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([\" \".join(doc) for doc in data[\"cleaned_text\"]])\n",
    "\n",
    "# Filter words in each transcript based on TF-IDF scores\n",
    "def filter_by_tfidf(doc):\n",
    "    features = tfidf_vectorizer.get_feature_names_out()\n",
    "    vector = tfidf_vectorizer.transform([\" \".join(doc)]).toarray()[0]\n",
    "    return [features[i] for i in vector.argsort()[-15:]]  # Top 15 tf-idf terms\n",
    "\n",
    "data[\"filtered_words\"] = data[\"cleaned_text\"].apply(filter_by_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import random\n",
    "\n",
    "# Assuming 'data' is a DataFrame that contains your preprocessed text data\n",
    "\n",
    "# Step 3: Train the LDA Model\n",
    "\n",
    "# Create dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(data[\"filtered_words\"])\n",
    "corpus = [dictionary.doc2bow(text) for text in data[\"filtered_words\"]]\n",
    "\n",
    "# General LDA model across all transcripts\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=42, passes=100)\n",
    "\n",
    "# Display general topics\n",
    "print(\"General Topics Across All Videos:\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Visualization for General Topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)\n",
    "\n",
    "# Create a WordCloud for general topics\n",
    "def create_wordcloud_for_topic(topic_model, topic_idx):\n",
    "    topic_words = topic_model.show_topic(topic_idx, topn=30)  # Get the top 30 words for the topic\n",
    "    word_freq = dict(topic_words)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"WordCloud for Topic {topic_idx}\")\n",
    "    plt.show()\n",
    "\n",
    "# Display WordClouds for general topics\n",
    "for i in range(5):  # Adjust to the number of topics\n",
    "    create_wordcloud_for_topic(lda_model, i)\n",
    "\n",
    "# Per-video LDA (randomly selecting 50 videos)\n",
    "video_topics = []\n",
    "max_videos = 50  # Limit to 50 videos\n",
    "\n",
    "# Get a random sample of video indices\n",
    "random_indices = random.sample(range(len(data[\"filtered_words\"])), min(max_videos, len(data[\"filtered_words\"])))\n",
    "\n",
    "for i in random_indices:\n",
    "    text = data[\"filtered_words\"][i]\n",
    "    corpus_per_video = [dictionary.doc2bow(text)]\n",
    "    lda_per_video = gensim.models.LdaModel(corpus=corpus_per_video, id2word=dictionary, num_topics=5, passes=100)\n",
    "    video_topics.append(lda_per_video.print_topics(-1))\n",
    "\n",
    "    # Prepare data for visualization for each video\n",
    "    vis_data_video = pyLDAvis.gensim_models.prepare(lda_per_video, corpus_per_video, dictionary)\n",
    "\n",
    "    # Create a WordCloud for each video's topics\n",
    "    for j in range(5):  # Adjust to the number of topics\n",
    "        create_wordcloud_for_topic(lda_per_video, j)\n",
    "\n",
    "# Show per-video topics\n",
    "for idx, video_topic in enumerate(video_topics):\n",
    "    print(f\"\\nTopics for Video {random_indices[idx] + 1}:\")  # Display the actual index + 1\n",
    "    for topic in video_topic:\n",
    "        print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Topic Validation and Coherence Checking\n",
    "\n",
    "# Coherence Score for General LDA Model\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data[\"filtered_words\"], dictionary=dictionary, coherence=\"c_v\")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(\"\\nGeneral LDA Model Coherence Score:\", coherence_lda)\n",
    "\n",
    "# Filter topics based on coherence threshold if necessary (e.g., only retain if > 0.5)\n",
    "if coherence_lda < 0.5:\n",
    "    print(\"Warning: Topic coherence is low, consider re-evaluating the topic terms or increasing the number of topics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Interpretability Verification\n",
    "\n",
    "def validate_topics_with_tags(tags, topics):\n",
    "    for idx, topic in enumerate(topics):\n",
    "        print(f\"\\nValidating Topic {idx+1}\")\n",
    "        keywords = [word.split('*')[1].strip('\"') for word in topic[1].split(\" + \")]\n",
    "        tag_matches = [tag for tag in tags.split() if tag in keywords]\n",
    "        print(\"Keywords:\", keywords)\n",
    "        print(\"Matching Tags:\", tag_matches)\n",
    "        if len(tag_matches) < len(keywords) * 0.3:\n",
    "            print(\"Warning: Topic may be too generic for this video.\")\n",
    "\n",
    "# Validate each video's topics with its tags\n",
    "for i, tags in enumerate(data[\"tags\"]):\n",
    "    print(f\"\\nValidating topics for Video {i+1}\")\n",
    "    validate_topics_with_tags(tags, video_topics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# Setup the summarization pipeline\n",
    "# Choose between T5 for abstractive or DistilBERT for extractive summarization\n",
    "# summarizer_abstractive = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\", framework=\"pt\", torch_dtype=\"float16\")\n",
    "# summarizer_extractive = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", tokenizer=\"sshleifer/distilbart-cnn-12-6\", framework=\"pt\")\n",
    "summarizer_abstractive = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", tokenizer=\"facebook/bart-large-cnn\", framework=\"pt\")\n",
    "# summarizer_abstractive = pipeline(\"summarization\", model=\"google/pegasus-xsum\", tokenizer=\"google/pegasus-xsum\", framework=\"pt\")\n",
    "\n",
    "# Path to the folder containing text files\n",
    "text_folder = 'cleaned_transcripts/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# Set the number of samples to process\n",
    "sample_size = 50\n",
    "def load_and_filter_text_files(folder_path, min_word_count=100):\n",
    "    file_paths = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    text_data = {}\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "                text = text.encode(\"utf-8\", \"ignore\").decode()  # Sanitize non-UTF-8 characters\n",
    "                if len(text.split()) >= min_word_count:\n",
    "                    text_data[file_path] = text\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file_path}: {e}\")\n",
    "    sampled_files = {k: text_data[k] for k in random.sample(list(text_data.keys()), min(len(text_data), sample_size))}\n",
    "    return sampled_files\n",
    "\n",
    "def chunk_text(text, chunk_size=512):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        yield \" \".join(words[i:i + chunk_size])\n",
    "\n",
    "def summarize_text(text, chunk_size=512, max_length=None, min_length=30, length_penalty=2.0):\n",
    "    summaries = []\n",
    "    try:\n",
    "        for chunk in chunk_text(text, chunk_size):\n",
    "            input_length = len(chunk.split())\n",
    "            if max_length is None or max_length > input_length:\n",
    "                max_length = min(input_length, 130)  # Set a reasonable max length based on the input length\n",
    "            summary = summarizer_abstractive(chunk, max_length=max_length, min_length=min_length, length_penalty=length_penalty, do_sample=False)\n",
    "            summaries.append(summary[0]['summary_text'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing text: {e}\")\n",
    "        return None\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "\n",
    "def parallel_summarization(text_data, chunk_size=512, max_workers=4):\n",
    "    summaries = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(summarize_text, text, chunk_size): file_name for file_name, text in text_data.items()}\n",
    "        for future in as_completed(futures):\n",
    "            file_name = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    summaries[file_name] = result\n",
    "                else:\n",
    "                    print(f\"No summary generated for {file_name}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your min_length=30 must be inferior than your max_length=16.\n",
      "d:\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1399: UserWarning: Unfeasible length constraints: `min_length` (30) is larger than the maximum possible length (16). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n",
      "Your min_length=30 must be inferior than your max_length=27.\n",
      "d:\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1399: UserWarning: Unfeasible length constraints: `min_length` (30) is larger than the maximum possible length (27). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization complete. Check the output folder for results.\n"
     ]
    }
   ],
   "source": [
    "# Load and summarize files\n",
    "text_data = load_and_filter_text_files(text_folder)\n",
    "summarized_texts = parallel_summarization(text_data, chunk_size=512)\n",
    "\n",
    "# Save summaries to output files\n",
    "output_folder = 'summaries/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for file_name, summary in summarized_texts.items():\n",
    "    base_name = os.path.basename(file_name)\n",
    "    output_path = os.path.join(output_folder, f\"{base_name}_summary.txt\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(summary)\n",
    "\n",
    "print(\"Summarization complete. Check the output folder for results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
