{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import requests\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import re\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures, TrigramCollocationFinder, TrigramAssocMeasures\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter, defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLP Models\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "#embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") faster\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\") \n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Fetch stopwords\n",
    "def fetch_stopwords_from_github(url):\n",
    "    response = requests.get(url)\n",
    "    return set(response.text.splitlines())\n",
    "\n",
    "github_stopwords_url = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt'\n",
    "github_stopwords = fetch_stopwords_from_github(github_stopwords_url)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = ['like', 'yeah', 'know', 'um', 'uh', 'really', 'one', 'go', 'right', 'okay', 'well', 'said', \n",
    "                    'going', 'got', 'na', 'always', 'every', 'each', 'say', 'el', 'little', 'still', \n",
    "                    'best', 'dutch', 'nice', 'great', 'awesome', 'good', 'cool', 'love', 'amazing', 'wow',\n",
    "                    'breaking news', 'report', 'coverage', 'investigation', 'interview', 'documentary', 'news', 'netherlands', 'psy', 'subtitle', 'description', 'link', \n",
    "                    'journalist', 'headline', 'reporter', 'current events', 'special report', \n",
    "                    'analysis', 'documented', 'broadcast', 'reporting', 'v', 'food', 'travel', 'react', \n",
    "                    'reacts', 'reaction', 'foreigner', 'thing', 'visit', 'dc', 'japan', 'first', 'fast', \n",
    "                    'asia', 'ang', 'indian', 'thai', 'vietnamese', 'russia', 'gon', 'canada', 'canadian', 'russian', \n",
    "                    'russia', 'guy', 'lot', 'bit', 'diba', 'ola', 'cuz', 'thai', 'thailand', 'person', 'citizen', 'foreigner', 'foreign', 'foreigners',\n",
    "                    'facebook', 'filipinos', 'filipinas', 'vlog', 'vlogs', 'vlogging', 'hashtag', 'india', 'bro', 'dito', 'people', 'time', 'music', 'gonna', 'life', \n",
    "                    'lol', 'guys', 'tho', 'cute', 'hmm', 'huh', 'channel', 'subscribe', 'day6', 'mandarin', 'chinese', 'beautiful',\n",
    "                    'chuckles', 'fbe', 'hit', 'laughs', 'yo', 'ka', 'word', 'living', 'boi', 'minimum', 'ya', 'successful', 'perfectly', 'yeap', \n",
    "                    'wondering', 'fantastic', 'hurry', 'german', 'age', 'country', 'subscribing', 'bluesy', 'jump', 'pretty', 'understanding', 'personalized',\n",
    "                    'and', 'the', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'up', 'about', 'over', 'into', 'through', 'between', 'under', 'against', 'all',\n",
    "                    'you', 'haha', 'hahaha', 'ha', 'hey', 'bye', 'hello', 'hi', 'oh', 'blah', 'easy', 'alright', 'ta', 'day', 'ooh', 'en', 'do', 'lot', 'comment', 'notification', \n",
    "                    'korean', 'jjajangmyeon', 'jajangmyeon', 'damn', 'yall', 'month', 'week', 'year', 'ohhh', 'pvf', 'dude', 'mmm', 'kagilagilalas', 'ofcourse', 'australia', 'uxo', \n",
    "                    'atleast', 'yusuf', 'bangkok', 'ot', 'anytime', 'allover', 'kala', 'nope', 'wan', 'brazil', 'smooth', 'ot', 'timeshere', 'batchof', 'yep', 'opo', 'del',\n",
    "                    'gosh', 'po', 'ourself', 'wo', 'wait', 'ugh', 'nyc', 'whoa', 'nicaragua', 'yup', 'em', 'bout', 'le', 'omg', 'overwhelm', 'maam', 'nicer', 'haha', 'hahaha', 'ha', \n",
    "                    'nbcs', 'lana', 'rc', 'whatsoever', 'oxy', 'decade', 'whyd', 'unknown', 'ahhhhh', 'ohoh', 'ohto', 'ohhhh', 'bruh', 'ooe', 'ahmedabad', 'mexico', \n",
    "                    'understand', 'excuse', 'kinda', 'applause', 'oooh', 'thiswhat', 'nevermind', 'ahh', 'againthank', 'toto', 'aww', 'nah', 'bbmas', 'ay', 'op', 'huh', 'huhu',\n",
    "                    'tada', 'beacuse', 'voila', 'upstairs', 'thatswhy', 'yea', 'that', 'armenia', 'or', 'not', 'funwhat', 'aka', 'armeniathat', 'woosexy', 'worth', 'laugh', 'box', \n",
    "                    'xd', 'vb', 'eff', 'ananya', 'welsh', 'latron', 'shout', 'whatwhat', 'what', 'pause', 'why', 'thats', 'byebye', 'iv', 'bye', 'ado', 'ownup', 'dom', 'jomm', 'sir', \n",
    "                    'budgie', 'nomac', 'lavocha', 'germany', 'why', 'walang', 'superduper', 'philip', 'mom', 'jre', 'giddy', 'intro', 'dupe', 'europe', 'dream', 'team', 'dislike', 'content', \n",
    "                    'yoongi', 'royale', 'ilu', 'jhope', 'day', 'jin', 'ecc', 'nyhs', 'nego', 'chavez', 'pb', 'everyones', 'epic', 'matter', 'oneonone', 'region', 'change', 'ho', 'seetoh', \n",
    "                    'atin', 'vpn', 'facetune', 'busu', 'mackie', 'clyd', 'china', 'rest', 'friend', 'woah', 'dindins', 'poster', 'vibe', 'woman', 'boss', 'woah', 'type', 'mahana', 'joke', \n",
    "                    'taller', 'insane', 'whang', 'psa', 'manatee', 'recommend', 'caesar', 'mmmhmm', 'mosul', 'dun', 'clue', 'naysayer', 'hindi', 'ko', 'pero', 'bulgaria', 'question', 'video', \n",
    "                    'yobi', 'hindu', 'expat', 'option', 'gap', 'eu', 'simo', 'kouignamann', 'bct', 'month', 'cfo', 'philippines', 'philippine', 'british', 'filipino', 'video', \n",
    "                    'http', 'korea', 'korean', 'youtube', 'google', 'united', 'america', 'american', 'kpop', '필리핀', 'bts', 'blackpink', 'twice', 'exo', 'k-pop', \n",
    "                    'seventeen', 'stray kids', 'nct', 'kdrama', 'aespa', 'taehyung', 'jimin', 'jungkook']\n",
    "stop_words.update(custom_stop_words, github_stopwords)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Folder paths\n",
    "transcripts_folder_path = 'final_transcripts/'\n",
    "tags_folder_path = 'video_tags/'\n",
    "\n",
    "# Function to load video tags only for fetched video IDs\n",
    "def load_video_tags(folder_path, video_ids):\n",
    "    video_tags = {}\n",
    "    for video_id in video_ids:\n",
    "        tag_file = os.path.join(folder_path, f\"{video_id}.txt\")\n",
    "        if os.path.exists(tag_file):\n",
    "            with open(tag_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                tags_content = file.read().lower()\n",
    "                video_tags[video_id] = tags_content.split()  # Store as list of words\n",
    "        else:\n",
    "            video_tags[video_id] = []  # Default to empty list if no tags\n",
    "    return video_tags\n",
    "\n",
    "video_ids = []\n",
    "transcript_files = []\n",
    "for file_name in os.listdir(transcripts_folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        video_id = file_name.split('_captions')[0]\n",
    "        video_ids.append(video_id)\n",
    "        transcript_files.append((video_id, file_name)) \n",
    "\n",
    "video_tags = load_video_tags(tags_folder_path, video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_latin_script(word):\n",
    "    return all('LATIN' in unicodedata.name(char, '') or char.isdigit() for char in word)\n",
    "\n",
    "# Function to detect both bigram and trigram collocations\n",
    "def detect_collocations(tokens, min_freq=3):\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    trigram_measures = TrigramAssocMeasures()\n",
    "\n",
    "    # Find bigrams\n",
    "    bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "    bigram_finder.apply_freq_filter(min_freq)\n",
    "    bigrams = set(['_'.join(bigram) for bigram in bigram_finder.nbest(bigram_measures.pmi, 10)])\n",
    "\n",
    "    # Find trigrams\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(tokens)\n",
    "    trigram_finder.apply_freq_filter(min_freq)\n",
    "    trigrams = set(['_'.join(trigram) for trigram in trigram_finder.nbest(trigram_measures.pmi, 10)])\n",
    "\n",
    "    return bigrams, trigrams\n",
    "\n",
    "def preprocess_text(doc, video_id, tag_weight=2, ngram_weight_factor=2):\n",
    "    # Segment the text into meaningful chunks\n",
    "    doc = re.sub(r'([a-zA-Z]+)[,;:!?.]', r'\\1', doc)\n",
    "    \n",
    "    bigram_trigram_words = []\n",
    "    \n",
    "    doc = doc.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(doc)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words and word.isalpha() and is_latin_script(word)]\n",
    "\n",
    "    # Detect meaningful bigram and trigram collocations\n",
    "    bigrams, trigrams = detect_collocations(tokens)\n",
    "\n",
    "    # Generate n-grams (both bigrams and trigrams)\n",
    "    bigram_tokens = ['_'.join(gram) for gram in ngrams(tokens, 2)]\n",
    "    trigram_tokens = ['_'.join(gram) for gram in ngrams(tokens, 3)]\n",
    "\n",
    "    # Count n-gram frequency\n",
    "    bigram_frequencies = Counter(bigram_tokens)\n",
    "    trigram_frequencies = Counter(trigram_tokens)\n",
    "\n",
    "    # Merge n-grams into single tokens\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens) - 2:  # Check for trigrams first\n",
    "        trigram = f\"{tokens[i]}_{tokens[i+1]}_{tokens[i+2]}\"\n",
    "        bigram = f\"{tokens[i]}_{tokens[i+1]}\"\n",
    "\n",
    "        if trigram in trigrams:\n",
    "            merged_tokens.append(trigram)\n",
    "            bigram_trigram_words.append(trigram)\n",
    "            i += 3  # Skip next two words since it's part of the trigram\n",
    "        elif bigram in bigrams:\n",
    "            merged_tokens.append(bigram)\n",
    "            bigram_trigram_words.append(bigram)\n",
    "            i += 2  # Skip next word since it's part of the bigram\n",
    "        else:\n",
    "            merged_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "\n",
    "    # Append any remaining words\n",
    "    while i < len(tokens):\n",
    "        merged_tokens.append(tokens[i])\n",
    "        i += 1\n",
    "\n",
    "    # POS tagging\n",
    "    tokens_with_pos = pos_tag(merged_tokens)\n",
    "\n",
    "    # Apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tokens_with_pos]\n",
    "\n",
    "    # Assign weight based on n-gram occurrence\n",
    "    weighted_tokens = []\n",
    "    for token in lemmatized_tokens:\n",
    "        if token in trigram_frequencies:\n",
    "            token_weight = 1 + trigram_frequencies[token] * ngram_weight_factor  \n",
    "        elif token in bigram_frequencies:\n",
    "            token_weight = 1 + bigram_frequencies[token] * (ngram_weight_factor - 1)  \n",
    "        else:\n",
    "            token_weight = 1\n",
    "        weighted_tokens.extend([token] * int(token_weight))\n",
    "\n",
    "    # Include video tags\n",
    "    if video_id in video_tags:\n",
    "        tags = video_tags[video_id]\n",
    "        for tag in tags:\n",
    "            if tag.isalpha():\n",
    "                weighted_tokens.extend([tag] * int(tag_weight))\n",
    "\n",
    "    return ' '.join(weighted_tokens), bigram_trigram_words\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert POS tag to WordNet format for lemmatization.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "def plot_wordclouds(lda_model, num_words=30):\n",
    "    topics = [lda_model.show_topic(i, num_words) for i in range(lda_model.num_topics)]\n",
    "    non_empty_topics = [t for t in topics if t]  # Filter out empty topics\n",
    "    num_topics = len(non_empty_topics)\n",
    "\n",
    "    if num_topics == 0:\n",
    "        print(\"No valid topics to display.\")\n",
    "        return\n",
    "\n",
    "    # Determine number of rows dynamically\n",
    "    cols = 3  # Fixed number of columns\n",
    "    rows = (num_topics // cols) + (1 if num_topics % cols else 0)  # Adjust rows based on topics\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(15, 5 * rows))\n",
    "\n",
    "    # Flatten axes for easier iteration\n",
    "    axes = axes.flatten() if num_topics > 1 else [axes]\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < num_topics:\n",
    "            words = dict(non_empty_topics[i])  # Get words and their weights\n",
    "            wordcloud = WordCloud(width=400, height=300, background_color='white').generate_from_frequencies(words)\n",
    "\n",
    "            ax.imshow(wordcloud, interpolation='bilinear')\n",
    "            ax.set_title(f\"Topic {i}\", fontsize=14)\n",
    "            ax.axis(\"off\")\n",
    "        else:\n",
    "            ax.axis(\"off\")  # Hide unused subplots\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.3)  # Reduce spacing between rows\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot a bar chart for the number of videos per topic\n",
    "def plot_topic_distribution(topic_counts):\n",
    "    topics, counts = zip(*sorted(topic_counts.items()))  # Get topic numbers and counts\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(topics, counts, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel(\"Topic ID\")\n",
    "    plt.ylabel(\"Number of Videos\")\n",
    "    plt.title(\"Number of Videos Per Topic\")\n",
    "    plt.xticks(topics)  # Set topic labels on x-axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = []\n",
    "preprocessed_text = []\n",
    "bigram_trigram_text = {}\n",
    "\n",
    "for video_id, file_name in transcript_files:\n",
    "    with open(os.path.join(transcripts_folder_path, file_name), 'r', encoding='utf-8') as file:\n",
    "        content = file.read().lower()\n",
    "        if len(content.split()) >= 100: \n",
    "            processed_text, bigram_trigram = preprocess_text(content, video_id)  # Get both processed text and segments\n",
    "            preprocessed_text.append((video_id, processed_text))\n",
    "            all_documents.append(processed_text)\n",
    "            bigram_trigram_text[video_id] = bigram_trigram\n",
    "\n",
    "# Create Dictionary and Corpus\n",
    "dictionary = corpora.Dictionary([doc.split() for doc in all_documents])\n",
    "corpus = [dictionary.doc2bow(doc.split()) for doc in all_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA Model\n",
    "lda_model_12 = LdaModel(corpus, num_topics=20, id2word=dictionary, alpha='auto', eta='auto', passes=100)\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=lda_model_12, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")\n",
    "\n",
    "# Print Topics\n",
    "topics = lda_model_12.print_topics(num_words=20)\n",
    "for topic_id, topic_words in topics:\n",
    "    print(f\"Topic {topic_id}: {topic_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_videos = defaultdict(list)\n",
    "\n",
    "video_topic_mapping_12 = {}\n",
    "\n",
    "# probability threshold for assigning multiple topics\n",
    "prob_threshold = 0.4\n",
    "\n",
    "# Dictionary to store topic words for each video\n",
    "video_topic_words_LDA1_12 = {}\n",
    "\n",
    "for idx, doc_bow in enumerate(corpus):\n",
    "    video_id = video_ids[idx]  # Get video ID\n",
    "    topic_distribution = lda_model_12.get_document_topics(doc_bow, minimum_probability=0)\n",
    "\n",
    "    # Get topics where probability is above threshold\n",
    "    assigned_topics = [topic for topic, prob in topic_distribution if prob >= prob_threshold]\n",
    "    video_topic_mapping_12[video_id] = assigned_topics  # Store assigned topics per video\n",
    "\n",
    "    for topic in assigned_topics:\n",
    "        topic_to_videos[topic].append(video_id)\n",
    "\n",
    "    # Get the representative words for each assigned topic\n",
    "    topic_words = []\n",
    "    for topic in assigned_topics:\n",
    "        words = [word for word, _ in lda_model_12.show_topic(topic, topn=30)]  # Get top 10 words\n",
    "        topic_words.append(\", \".join(words))  # Convert list to string\n",
    "\n",
    "    # Store the topic words as a string\n",
    "    video_topic_words_LDA1_12[video_id] = \"; \".join(topic_words)  # Separate topics with `;`\n",
    "\n",
    "# Count occurrences of each topic\n",
    "topic_counts = Counter()\n",
    "\n",
    "for topics in video_topic_mapping_12.values():\n",
    "    for topic in topics:\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "# Print the number of videos per topic\n",
    "print(\"\\nNumber of Videos Per Topic:\")\n",
    "for topic, count in sorted(topic_counts.items()):\n",
    "    print(f\"Topic {topic}: {count} videos\")\n",
    "\n",
    "# Print topics assigned per video\n",
    "print(\"\\nTopics Assigned Per Video:\")\n",
    "for video_id, topics in video_topic_mapping_12.items():\n",
    "    topic_list = ', '.join(map(str, topics)) if topics else \"No dominant topic\"\n",
    "    print(f\"Video ID: {video_id} → Topics: {topic_list}\")\n",
    "\n",
    "# Print videos per topic\n",
    "print(\"\\nTop Words Per Topic:\")\n",
    "num_words = 30  \n",
    "\n",
    "for topic_id in sorted(topic_to_videos.keys()): \n",
    "    top_words = lda_model_12.show_topic(topic_id, num_words)\n",
    "    words_str = ', '.join([word for word, prob in top_words])\n",
    "    print(f\"Topic {topic_id}: {words_str}\")\n",
    "\n",
    "topic_word_contributions = {}\n",
    "for idx, doc_bow in enumerate(corpus):\n",
    "    video_id = video_ids[idx]\n",
    "    topic_distribution = lda_model_12.get_document_topics(doc_bow, minimum_probability=0)\n",
    "    assigned_topics = [topic for topic, prob in topic_distribution if prob >= prob_threshold]\n",
    "    video_topic_mapping_12[video_id] = assigned_topics\n",
    "    for topic in assigned_topics:\n",
    "        topic_to_videos[topic].append(video_id)\n",
    "    topic_word_contributions[video_id] = {topic: lda_model_12.show_topic(topic, topn=30) for topic in assigned_topics}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
